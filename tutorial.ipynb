{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aacaa3b-ecf0-4ed0-bf61-02dbd463513a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scaling up Data Science Workflow/Pipeline with Ray Environment Through NOAA-GHCN Dataset\n",
    "\n",
    "In this tutorial, we will be using primarily NumS, Modin, Sklearn, and Ray to show how we can **scale** dataframe manipulation and classical machine learning algorithms to analyze a big dataset. \n",
    "\n",
    "Once the autoscaler script has finished running and initialzing, we can start running cells to demonstrate the scalability. We'll explain the scalability as well as the high interpretability we can gain from classical machine learning models.\n",
    "\n",
    "We will split up the notebook into 5 main sections:\n",
    "* Setting Up and Loading Datasets with Modin and S3\n",
    "* Dataframe Manipulation and Cleaning with Modin\n",
    "* Visualizations and Understanding the Data\n",
    "* Modeling Weather by Space within Ray Environment (NumS, XGBoost, Sklearn)\n",
    "* Modeling Weather by Time with Time Series (NumS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc3b89-6acf-4250-b9ad-6f2f1b0d3ebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting Up and Loading Datasets with Modin and S3\n",
    "Here are the imports for our libraries. The main imports are NumS, Modin, Sklearn, and Ray. There are also additional libraries such as NumPy and Pandas to compare performance as well as to support other libraries aren't compatibile with NumS and Modin. We will have to use intermediate calls to transform them into the correct type/data structure (e.g. Sklearn only accepts NumPy arrays).\n",
    "\n",
    "The most important thing after setting up Ray is to make sure that the Ray Python package can also recognize and connect to the cluster setup if there is one. Uncommenting the second line will enable single node setup, while uncommenting the third line will enable cluster setup. Additionally, the cell below where NumS cluster shape is configured will also confirm whether or not there are head and worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf3389c-ed6d-4ef2-99a6-613fec14ce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-04 08:58:54,430\tINFO services.py:1265 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "ray.init(ignore_reinit_error=True, num_cpus=32, _temp_dir=\"/home/brian/external/aws-asdi/ray_temp\"); # ray.init() config for single node setup\n",
    "#ray.init(ignore_reinit_error=True, address=\"auto\", _redis_password='5241590000000000'); # ray.init() config for cluster setup\n",
    "import modin.pandas as pd\n",
    "import pandas\n",
    "from nums import numpy as nps\n",
    "from nums.core import settings\n",
    "from nums.experimental import nums_modin\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import warnings\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import gzip\n",
    "import sys\n",
    "from io import StringIO\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926ba9bc-d4c5-4646-b27e-a067df680bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumS settings\n",
    "if len(ray.nodes()) > 1:\n",
    "    settings.cluster_shape = (len(ray.nodes())-1, 1)\n",
    "settings.cluster_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657ee78-2b24-49d1-8c69-0b904d6c8000",
   "metadata": {},
   "source": [
    "Next, we will load some datasets to give an idea how fast Modin compared to Pandas. Not only will we be comparing on a single node, we will also show how much more speed you can achieve at scale in Modin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5eebce3-9d03-421b-b103-0fc4bf2ae4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 262 ms, sys: 47 ms, total: 309 ms\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climate_2020 = pd.read_csv('data/2020.csv', header=None, names=[\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"], quoting=3)\n",
    "climate_2020[\"YEAR/MONTH/DAY\"] = pd.to_datetime(climate_2020[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "del climate_2020 # delete from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe52ca4-7771-4757-8d6c-86f3fd94d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 3.06 s, total: 17.8 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climate_2020 = pandas.read_csv('data/2020.csv', header=None, names=[\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"], quoting=3)\n",
    "climate_2020[\"YEAR/MONTH/DAY\"] = pd.to_datetime(climate_2020[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "del climate_2020 # delete from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18205ad0-42df-45dd-b095-f84b9703be3b",
   "metadata": {},
   "source": [
    "Additionally, we can also directly download from S3 bucket links. But given the connection speeds, performance may be inconsistent. It is reccomended to be near `us-east-1` server, as the public S3 bucket is located there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f5f665-14c3-4eda-b08a-884dca59e047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 447 ms, total: 2.55 s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climate_2020 = pd.read_csv('s3://noaa-ghcn-pds/csv/2020.csv', header=None, names=[\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"], quoting=3)\n",
    "climate_2020[\"YEAR/MONTH/DAY\"] = pd.to_datetime(climate_2020[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "del climate_2020 # delete from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5dfa113-c834-465f-a59e-2d49c7fb3574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 3.58 s, total: 26.1 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climate_2020 = pandas.read_csv('s3://noaa-ghcn-pds/csv/2020.csv', header=None, names=[\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"], quoting=3)\n",
    "climate_2020[\"YEAR/MONTH/DAY\"] = pd.to_datetime(climate_2020[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "del climate_2020 # delete from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa6a3f-22e9-4adf-87d1-74818e99a175",
   "metadata": {},
   "source": [
    "## Dataframe Manipulation and Cleaning with Modin\n",
    "Next, we will show how we can do dataframe manipulation to understand the data we are dealing with. We have downloaded and loaded onto memory the NOAA Global Historical Climatology Network Daily (GHCN-D) dataset which containes over 200+ years worth of historical weather data. Data consists of `csv` files dating back to 1763, majority of logs containing temperature maximum, temperature minimum, precipitation, and snowfall. We can first start off by showing the inventory log of each station by reading `noaa-ghcn-pds/ghcnd-inventory.txt` directly from S3, showing the years of data as well as the element of weather a station has recorded.\n",
    "\n",
    "Sources:\n",
    "* [Data](https://registry.opendata.aws/noaa-ghcn/)\n",
    "* [Documentation](https://registry.opendata.aws/noaa-ghcn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acdfb85d-d5cf-4600-a6e8-00ab87b68b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>FIRSTYEAR</th>\n",
       "      <th>LASTYEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705035</th>\n",
       "      <td>ZI000067983</td>\n",
       "      <td>-20.2000</td>\n",
       "      <td>32.6160</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>1951</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705036</th>\n",
       "      <td>ZI000067983</td>\n",
       "      <td>-20.2000</td>\n",
       "      <td>32.6160</td>\n",
       "      <td>TAVG</td>\n",
       "      <td>1962</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705037</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1951</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705038</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>1951</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705039</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>1951</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>705040 rows x 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  LATITUDE  LONGITUDE ELEMENT  FIRSTYEAR  LASTYEAR\n",
       "0       ACW00011604   17.1167   -61.7833    TMAX       1949      1949\n",
       "1       ACW00011604   17.1167   -61.7833    TMIN       1949      1949\n",
       "2       ACW00011604   17.1167   -61.7833    PRCP       1949      1949\n",
       "3       ACW00011604   17.1167   -61.7833    SNOW       1949      1949\n",
       "4       ACW00011604   17.1167   -61.7833    SNWD       1949      1949\n",
       "...             ...       ...        ...     ...        ...       ...\n",
       "705035  ZI000067983  -20.2000    32.6160    PRCP       1951      2020\n",
       "705036  ZI000067983  -20.2000    32.6160    TAVG       1962      2020\n",
       "705037  ZI000067991  -22.2170    30.0000    TMAX       1951      1990\n",
       "705038  ZI000067991  -22.2170    30.0000    TMIN       1951      1990\n",
       "705039  ZI000067991  -22.2170    30.0000    PRCP       1951      1990\n",
       "\n",
       "[705040 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventory = pd.read_fwf('s3://noaa-ghcn-pds/ghcnd-inventory.txt', widths=[12, 9, 10, 4, 5, 5], header=None, names=[\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEMENT\", \"FIRSTYEAR\", \"LASTYEAR\"])\n",
    "inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd93f5-1c4a-43a4-9102-9c571d53d35b",
   "metadata": {},
   "source": [
    "We can also read `noaa-ghcn-pds/ghcnd-stations.txt` to output the stations to get more metadata such as elevation and the name of the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0a9550-0c49-484e-8501-e766f1e52ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: File has mismatched quotes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN FLAG</th>\n",
       "      <th>HCN/CRN FLAG</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>10.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>19.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.5170</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.3640</td>\n",
       "      <td>10.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.6510</td>\n",
       "      <td>26.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118488</th>\n",
       "      <td>ZI000067969</td>\n",
       "      <td>-21.0500</td>\n",
       "      <td>29.3670</td>\n",
       "      <td>861.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WEST NICHOLSON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118489</th>\n",
       "      <td>ZI000067975</td>\n",
       "      <td>-20.0670</td>\n",
       "      <td>30.8670</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MASVINGO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118490</th>\n",
       "      <td>ZI000067977</td>\n",
       "      <td>-21.0170</td>\n",
       "      <td>31.5830</td>\n",
       "      <td>430.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUFFALO RANGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118491</th>\n",
       "      <td>ZI000067983</td>\n",
       "      <td>-20.2000</td>\n",
       "      <td>32.6160</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHIPINGE</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118492</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>457.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BEITBRIDGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67991.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118493 rows x 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0       ACW00011604   17.1167   -61.7833       10.1   NaN   \n",
       "1       ACW00011647   17.1333   -61.7833       19.2   NaN   \n",
       "2       AE000041196   25.3330    55.5170       34.0   NaN   \n",
       "3       AEM00041194   25.2550    55.3640       10.4   NaN   \n",
       "4       AEM00041217   24.4330    54.6510       26.8   NaN   \n",
       "...             ...       ...        ...        ...   ...   \n",
       "118488  ZI000067969  -21.0500    29.3670      861.0   NaN   \n",
       "118489  ZI000067975  -20.0670    30.8670     1095.0   NaN   \n",
       "118490  ZI000067977  -21.0170    31.5830      430.0   NaN   \n",
       "118491  ZI000067983  -20.2000    32.6160     1132.0   NaN   \n",
       "118492  ZI000067991  -22.2170    30.0000      457.0   NaN   \n",
       "\n",
       "                         NAME GSN FLAG HCN/CRN FLAG   WMO ID  \n",
       "0       ST JOHNS COOLIDGE FLD      NaN          NaN      NaN  \n",
       "1                    ST JOHNS      NaN          NaN      NaN  \n",
       "2         SHARJAH INTER. AIRP      GSN          NaN  41196.0  \n",
       "3                  DUBAI INTL      NaN          NaN  41194.0  \n",
       "4              ABU DHABI INTL      NaN          NaN  41217.0  \n",
       "...                       ...      ...          ...      ...  \n",
       "118488         WEST NICHOLSON      NaN          NaN  67969.0  \n",
       "118489               MASVINGO      NaN          NaN  67975.0  \n",
       "118490          BUFFALO RANGE      NaN          NaN  67977.0  \n",
       "118491               CHIPINGE      GSN          NaN  67983.0  \n",
       "118492             BEITBRIDGE      NaN          NaN  67991.0  \n",
       "\n",
       "[118493 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations = pd.read_fwf('s3://noaa-ghcn-pds/ghcnd-stations.txt', widths=[12, 9, 10, 7, 3, 31, 4, 4, 6], header=None, names=[\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"STATE\", \"NAME\", \"GSN FLAG\", \"HCN/CRN FLAG\", \"WMO ID\"])\n",
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762af1c-5763-4470-87d2-746f640a9026",
   "metadata": {},
   "source": [
    "Additionally, the ID per each station encodes its country in `noaa-ghcn-pds/ghcnd-countries.txt`. This will be useful later on if we want to do an analysis of data per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dbbe06a-b4f9-4160-b828-ae0a0118dabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `Series.<lambda>` defaulting to pandas implementation.\n",
      "To request implementation, send an email to feature_requests@modin.org.\n",
      "UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AC</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AG</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AJ</td>\n",
       "      <td>Azerbaijan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>WI</td>\n",
       "      <td>Western Sahara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>WQ</td>\n",
       "      <td>Wake Island [United States]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>WZ</td>\n",
       "      <td>Swaziland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>ZA</td>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>ZI</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows x 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    code                       country\n",
       "0     AC          Antigua and Barbuda \n",
       "1     AE         United Arab Emirates \n",
       "2     AF                   Afghanistan\n",
       "3     AG                      Algeria \n",
       "4     AJ                   Azerbaijan \n",
       "..   ...                           ...\n",
       "214   WI               Western Sahara \n",
       "215   WQ   Wake Island [United States]\n",
       "216   WZ                    Swaziland \n",
       "217   ZA                       Zambia \n",
       "218   ZI                     Zimbabwe \n",
       "\n",
       "[219 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes = pd.read_csv(\"s3://noaa-ghcn-pds/ghcnd-countries.txt\", delimiter=\"\\n\", header=None)[0].str.extract('(?P<code>.{2})(?P<country>.{0,})')\n",
    "country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c990751b-6f2a-437d-ac10-d7289da48e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_loader(year, local=False):\n",
    "    if local:\n",
    "        df = pd.read_csv('data/' + str(year) + '.csv', header=None, names=[\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"], quoting=3)\n",
    "    else:\n",
    "        df = pd.read_csv('s3://noaa-ghcn-pds/csv/' + str(year) + '.csv', header=None, names=[\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"], quoting=3)\n",
    "    df[\"YEAR/MONTH/DAY\"] = pd.to_datetime(df[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "    return df\n",
    "\n",
    "def df_filter(df, _id, element):\n",
    "    return df.loc[(df[\"ID\"] == _id) & (df[\"ELEMENT\"] == element)][[\"YEAR/MONTH/DAY\", \"DATA VALUE\"]].set_index(\"YEAR/MONTH/DAY\").sort_index()\n",
    "\n",
    "# Same as df_filter(), but a vector of ALL the data\n",
    "def df_filter_vector(_id, element, local=False, custom_years=None):\n",
    "    df_vector = pd.DataFrame(columns=[\"DATA VALUE\"])\n",
    "    if custom_years:\n",
    "        years = range(custom_years[0], custom_years[1] + 1)\n",
    "    for year in tqdm(years):\n",
    "        if local:\n",
    "            df = df_filter(dfs[year], _id, element)\n",
    "        else:\n",
    "            df = df_filter(df_loader(year), _id, element)\n",
    "            \n",
    "        if df_vector.empty:\n",
    "            df_vector = df\n",
    "        else:\n",
    "            df_vector = df_vector.append(df)\n",
    "    return df_vector\n",
    "\n",
    "def rmse(actual, expected):\n",
    "    \"\"\"\n",
    "    Computes the root mean squared error to evaluate models/predictions. It can accept all the datatypes used\n",
    "    in this notebook\n",
    "    \"\"\"\n",
    "    if type(actual) != type(expected):\n",
    "        raise TypeError(\"actual and expected must be the same types\")\n",
    "    if type(actual) == np.ndarray:\n",
    "        return np.sqrt(np.mean((expected - actual) ** 2))\n",
    "    elif type(actual) == type(nps.array([])):\n",
    "        return nps.sqrt(nps.mean((expected - actual) ** 2)).get()\n",
    "    elif type(actual) == modin.pandas.dataframe.DataFrame or type(actual) == pandas.core.frame.DataFrame:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise TypeError\n",
    "        \n",
    "def design_matrix(years, elements, target=None, convert_nps=False, local=False):\n",
    "    \"\"\"\n",
    "    Set target to your \"y\" predictor. If y has NaNs or missing values, we will drop the data row.\n",
    "    \"\"\"\n",
    "    df_design = pd.DataFrame()\n",
    "    \n",
    "    for year in tqdm(years):\n",
    "        if local:\n",
    "            df = dfs[year]\n",
    "        else:\n",
    "            df = df_loader(year)\n",
    "            \n",
    "\n",
    "        if target[0] not in df[\"ELEMENT\"].unique():\n",
    "            continue\n",
    "\n",
    "        df = df[df['ELEMENT'].isin(elements)]\n",
    "        df = pd.pivot_table(df, index=[\"ID\", \"YEAR/MONTH/DAY\"], columns=\"ELEMENT\", values=\"DATA VALUE\").reset_index(level=[0,1])\n",
    "        df = df.merge(stations[[\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"]], how='inner', on='ID')\n",
    "        \n",
    "        if target:\n",
    "            df = df.dropna(subset=target)\n",
    "        df = df.dropna() #TODO add a flag to toggle this\n",
    "        \n",
    "        \n",
    "        df[\"YEAR/MONTH/DAY\"] = df[\"YEAR/MONTH/DAY\"].apply(lambda x: pd.Period(x, freq='D').day_of_year)\n",
    "        df[\"TMAX\"] = df[\"TMAX\"] / 10\n",
    "        df[\"TMIN\"] = df[\"TMIN\"] / 10\n",
    "        df[\"AVG\"] = (df[\"TMAX\"] + df[\"TMIN\"]) / 2\n",
    "        df[\"RANGE\"] = df[\"TMAX\"] - df[\"TMIN\"]\n",
    "        #df = df.drop([\"ID\"], axis=1)\n",
    "        #df = df.astype(float)\n",
    "        \n",
    "        if df_design.empty:\n",
    "            df_design = df\n",
    "        else:\n",
    "            df_design = df_design.append(df)\n",
    "        \n",
    "        \n",
    "    \n",
    "    if convert_nps:\n",
    "        return nps.array(result.to_numpy().astype(np.double))\n",
    "    return df_design\n",
    "\n",
    "def design_matrix_time_series_stack(_id, element, years, convert_nps=True, local=False):\n",
    "    \"\"\"\n",
    "    Inputs are station ID and element\n",
    "    Output is a design matrix of time series per year stacked on top of each other.\n",
    "    rows are year of data collected, \n",
    "    cols are day of the year\n",
    "    \n",
    "    returns NumS array or Pandas DataFrame\n",
    "    \"\"\"\n",
    "    df_design = pd.DataFrame(columns=pd.date_range(start=\"2020-01-01\", end=\"2020-12-31\").strftime('%m-%d'))\n",
    "    station_name = stations.loc[stations['ID'] == _id][\"NAME\"].item()\n",
    "    \n",
    "    for year in tqdm(years):\n",
    "        if local:\n",
    "            df = dfs[year]\n",
    "        else:\n",
    "            try:\n",
    "                df = df_loader(year)\n",
    "            except ClientError:\n",
    "                tqdm.write(str(year) + \".csv doesn't exist on remote, addition to design matrix is skipped.\")\n",
    "                continue\n",
    "            \n",
    "        df = df_filter(df, _id, element)\n",
    "        if df.empty:\n",
    "            tqdm.write(element + \" data on \" + str(year) + \" for \" + station_name + \" with id: \" + _id + \" is empty. Addition to design matrix is skipped.\")\n",
    "            continue\n",
    "        df.index = df.index.strftime('%m-%d')\n",
    "        df.columns = [year]\n",
    "        df = df.T\n",
    "        df_design = df_design.append(df)\n",
    "\n",
    "\n",
    "    df_design.index.name = None\n",
    "    if convert_nps:\n",
    "        #return nums_modin.from_modin(df_design) # Experimental version has some bugs, use manual conversion for now\n",
    "        return nps.array(df_design.to_numpy().astype(np.double))\n",
    "    return df_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05a0763-1d9f-4d74-9578-28bb848932c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "elements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "all_elements = list(inventory[\"ELEMENT\"].unique())\n",
    "years = list(range(1763, 2022))\n",
    "local = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9c41a-7990-4da1-831c-322ff3d21c1c",
   "metadata": {},
   "source": [
    "Loading all the dataframes from local storage to memory gives us these times for a single node:\n",
    "\n",
    "**modin**:\n",
    "```\n",
    "CPU times: user 35.5 s, sys: 7.8 s, total: 43.3 s\n",
    "Wall time: 5min 20s\n",
    "```\n",
    "\n",
    "**pandas**\n",
    "```\n",
    "CPU times: user 24min 22s, sys: 5min 39s, total: 30min 2s\n",
    "Wall time: 29min 32s\n",
    "```\n",
    "\n",
    "As we can see, Modin gives us ~6x speedup, which is impressive considering that ~100GB is being transferred and loaded onto memory.\n",
    "\n",
    "In the Ray cluster setup, the speed up is even more with ~15x speedup (1 head, 4 workers):\n",
    "```\n",
    "CPU times: user 2min 50s, sys: 42.3 s, total: 3min 33s\n",
    "Wall time: 1min 59s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1caeec7-9109-4b40-a55b-d7773b42f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'NoneType'> object. This may take some time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e3e2edaf9f4e99899987940e71b2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "dfs = [pd.DataFrame() for _ in range(2022)]\n",
    "for year in tqdm(years):\n",
    "    dfs[year] = df_loader(year, local=local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5788c-92e0-46fd-b653-0ebe22d85978",
   "metadata": {},
   "source": [
    "Now that we have quickly loaded our dataframes, we can further our analysis of the data through Pandas API calls through Modin. Modin is able to abstract away all the workings between distributed memory and communication between worker nodes to give us high performance in dataframe manipulation. In the next cell, we can plot a dataframe that stores the countries in columns and the year in the rows. Each cell contains the number of data points each country has collected in it's weather stations (regardless of what element it is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44096c5c-7da3-4f2e-917a-b1dc5d0678c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data_freq = pd.DataFrame()\n",
    "years.reverse() # Reverse, so NaNs can be filled in to missing/old data\n",
    "\n",
    "for year in tqdm(years):\n",
    "    station_data_freq[year] = dfs[year].groupby(dfs[year][\"ID\"].str.slice(stop=2))[\"DATA VALUE\"].sum()\n",
    "\n",
    "years.reverse()\n",
    "station_data_freq = station_data_freq.fillna(0).T\n",
    "station_data_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860ddd8-36c5-4024-a1e9-a68051b73d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes.iterrows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b3af5-23aa-475f-8ddd-917ee2840886",
   "metadata": {},
   "source": [
    "We can also use this dataframe to plot out the growth of data points over the years. As earth becomes more civilized and technologically advanced, there is more data created and recorded. We see this observation here, especially for US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70e78e-ffc0-41c9-a9cf-f74c938d00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(country_codes.iterrows()): \n",
    "    if row[0] in station_data_freq.T.columns:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = station_data_freq.T[row[0]].plot();\n",
    "        ax.set_xlabel(\"Years\")\n",
    "        ax.set_ylabel(\"Number of Data Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf76df-f58f-4c0a-8101-d6b04e52904c",
   "metadata": {},
   "source": [
    "## Visualizations and Understanding the Data\n",
    "In addition to fast and scalable dataframe manipulation and wrangling, we can also use Matplotlib to seamlessly plot directly from Modin dataframes. The benefit of plotting and using all the data points rather than sampling is that we can see more detail and have definitive view of our data.\n",
    "We can also do a yearly global analysis of how many data points has been collected. Already, we can see near the 1950s is when weather data collection rapidly increased. The only downside to this visualization is that Matplotlib is not scalable and will run on the head node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d6d1d-87d6-41db-87a7-3e72882fafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points = []\n",
    "for year in tqdm(years):\n",
    "    sample_points.append(len(dfs[year].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492e6ef-4a39-4bc7-a5a4-09d6dba68585",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot(years, sample_points);\n",
    "plt.title(\"Number of Data Points Collected Per Year\")\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Data Points\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a438f-0e9f-4cda-9184-27d9c837230d",
   "metadata": {},
   "source": [
    "We can also observe what type or element of data has been recorded using simple dataframe calls. Here we plot the top 25 most popular ones, most popular being the precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d4911-e149-4792-a277-46dc06c56e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory[\"ELEMENT\"].value_counts().head(25).plot(kind=\"bar\");\n",
    "plt.title(\"Top Weather Data Type Collected\")\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b9a20-5ad9-4ec5-aab9-9891c0c5712c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also use the longitude and latitude provided to show the spatial frequency of where data is being collected, which reveals a very similar outline of our earth!\n",
    "\n",
    "We see that more developed countries and cities have higher spatial frequency of weather stations. It's also interesting to observe that inhabitable areas of land have lower or no weather stations. Some developed countries also have less scattered weather stations, such as China."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29cde3-3204-4e89-8662-3181da6b73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Scatterplot of Weather Stations\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.scatter(x=stations['LONGITUDE'], y=stations['LATITUDE'], s=0.9, alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7d528-fbef-4e37-9bcf-dafaa8ee894b",
   "metadata": {},
   "source": [
    "Although Matplotlib gives us a simple API to quickly plot graphs, with Modin seamlessly integrating with it, more advanced plotting libraries such as Plotly can give use better visualizations. We can use the metadata in our dataframes to our advantage and plot highly interpretable and visual plots on top of geographical maps provided by OpenStreetMap. First let's replicate the simple spatial plot we did in Matplotlib to Plotly on an overlay of the map.\n",
    "\n",
    "Due to data constraints, we will save it to a file called `station_density.html`. Again, Plotly is also not scalable, so it'll run on the head node. We wish to see more scalable visualization tools for data science someday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94d9ae-a3c0-4172-a138-8f12900cb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_mapbox(stations._to_pandas(), lat='LATITUDE', lon='LONGITUDE', radius=5,\n",
    "                        center=dict(lat=0, lon=180), zoom=0,\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"station_density.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a104e-2098-438b-ac2a-0a14c8b1002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = design_matrix([2010], [\"TMAX\", \"TMIN\"], target=[\"TMAX\"], local=True)\n",
    "df[\"TMAX\"] = df[\"TMAX\"]\n",
    "df[\"TMIN\"] = df[\"TMIN\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4917ea-97e2-48e9-a475-98b725f77370",
   "metadata": {},
   "source": [
    "## Modeling Weather by Space within Ray Environment (NumS, XGBoost, Sklearn)\n",
    "After exploratory data analysis, our next step is to answer or find another question to our problem and offer explanation and reasoning behind a solution or answer, primarily through modeling. With NumS, it's possible to do generalized linear modeling (GLM) to give us interpretable predictions and modeling at scale. We can also use Modin's experimental package of XGBoost to directly run XGBoost off of a Modin dataframe, a popular library for gradient boosted decision trees. And lastly, we can use SKlearn's Random Forest model with Ray by replacing the joblib backend with Ray actors.\n",
    "\n",
    "Before we start, let's define a few problems. We know that inhabitable areas have little to no stations or data recorded. Is it possible to \"learn\" the features of the earth's geographical location and time to predict weather? \n",
    "\n",
    "Let's first create a design matrix. Here, we will make a design matrix that one hot encodes the day to the day of the year, precipitation, temperature maximum, temperature minimum, temperature average, temperature range, latitude, longitude, and elevation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f6d4b-fbbe-41e8-951e-5e0ed1718e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_elements = [\"PRCP\", \"TMAX\", \"TMIN\"]\n",
    "data = design_matrix(years[-10:], test_elements, target=[\"PRCP\"], convert_nps=False, local=local)\n",
    "data = data[data[\"PRCP\"] >= 0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d8941-c82e-497f-a558-a0fef86c209a",
   "metadata": {},
   "source": [
    "Using our previous techniques of EDA, we can also plot the graph. Let's take a day such as Januaary 1st, 2020 and plot the precipitation onto our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1868f-3c43-45ea-931f-604cd1e075ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = design_matrix([2020], test_elements, target=[\"PRCP\"], convert_nps=False, local=local)\n",
    "plot = plot[plot[\"PRCP\"] >= 0]\n",
    "plot = plot[(plot[\"YEAR/MONTH/DAY\"] == 1) & (plot[\"PRCP\"] > 0)]\n",
    "fig = go.Figure(go.Densitymapbox(lat=plot.LATITUDE, lon=plot.LONGITUDE, z=plot.PRCP,\n",
    "                                 radius=10))\n",
    "fig.update_layout(mapbox_style=\"stamen-terrain\", mapbox_center_lon=180)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"prcp_dmatrix_density.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9998b1f-b083-47a0-b4bf-78a0136d2e0d",
   "metadata": {},
   "source": [
    "Next, we can perform a train-test split on the data. Because the data is so big, and there are multiple logs of repetitive data, doing a train-test split on raw data logs might take too long. Instead, we will train-test split based on station ID. This allows us to hold out a few stations for training, and allows us to do cross-validation to evaluate whether our model has \"learned\" the weather patterns of other stations from spatial and temporal locality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c011b98-5367-4aa1-a8bd-49f03bfaaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.random.permutation(data[\"ID\"].unique())\n",
    "split = int(ids.shape[0] * .8)\n",
    "train_ids = ids[:split]\n",
    "test_ids = ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192f3ba-1eaf-4b8b-a379-26bd9af652d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data[\"ID\"].isin(train_ids)]\n",
    "test = data[data[\"ID\"].isin(test_ids)]\n",
    "#'TMAX', 'TMIN','AVG', 'RANGE',\n",
    "X_train = train[['YEAR/MONTH/DAY', 'TMAX', 'TMIN','AVG', 'RANGE', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].to_numpy().astype(np.double)\n",
    "y_train = train['PRCP'].to_numpy().astype(np.double)\n",
    "X_test = test[['YEAR/MONTH/DAY', 'TMAX', 'TMIN','AVG', 'RANGE', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].to_numpy().astype(np.double)\n",
    "y_test = test['PRCP'].to_numpy().astype(np.double)\n",
    "\n",
    "# NumS arrays must maintain block shapes, code below ensures they are shaped correctly.\n",
    "X_train = nps.array(X_train)\n",
    "y_train = nps.array(y_train)\n",
    "X_test = nps.array(X_test)\n",
    "y_test = nps.array(y_test)\n",
    "y_train = y_train.reshape(block_shape=(X_train.block_shape[0],))\n",
    "y_test = y_test.reshape(block_shape=(X_test.block_shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f6720-091e-4688-9809-ccaa14cf4a43",
   "metadata": {},
   "source": [
    "We can start off with something simple, such as Linear Regression to see if any of the features have a linear relationship. NumS provides a Sklearn-ish API through the `nums.models` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9f07d-3e36-4b91-99da-4df5c621b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.models.glms import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32beaa-4a05-40f5-ab2e-e0fefdc7d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = model.predict(X_train).get()\n",
    "test_results = model.predict(X_test).get()\n",
    "\n",
    "print(\"Training RMSE:\", rmse(training_results, y_train.get()))\n",
    "print(\"Testing RMSE\", rmse(test_results, y_test.get()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562604c-a485-4157-bca7-8f033b8deb09",
   "metadata": {},
   "source": [
    "As we can see from the RMSE values, they do not look promising. We can also use Logistic Regression to model the precipitation in our earth. First, it'll require us to change our prediciton values to prediction labels for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ac51f-4557-4641-a63e-db1283befc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_train and y_test to binary labels\n",
    "y_train_block_shape = y_train.block_shape\n",
    "y_train = y_train.get()\n",
    "y_train[y_train > 0] = 1.0\n",
    "y_train = nps.array(y_train)\n",
    "y_train = y_train.reshape(block_shape=y_train_block_shape)\n",
    "\n",
    "\n",
    "y_test_block_shape = y_test.block_shape\n",
    "y_test = y_test.get()\n",
    "y_test[y_test > 0] = 1.0\n",
    "y_test = nps.array(y_test)\n",
    "y_test = y_test.reshape(block_shape=y_test_block_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1588a1-3327-426e-a799-5951934e2ef8",
   "metadata": {},
   "source": [
    "We will also see that if our matrix is singular. As a fix, we will add a small mask of floating point numbers to make it invertible without drastically changing the numbers of our design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fd564-2c5b-4556-93ec-d0c30478f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.models.glms import LogisticRegression\n",
    "\n",
    "# since matrix could be singular, a mask is added on to X_train to help invert it\n",
    "mask = nps.random.rand(X_train.shape[0], X_train.shape[1]) * 0.00001\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train + mask, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb956b5-a73d-4ada-bd0f-9368d5a85561",
   "metadata": {},
   "source": [
    "We get some values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2c97b-55eb-4b46-a08c-19d7505afe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = model.predict(X_train).get()\n",
    "test_results = model.predict(X_test).get()\n",
    "\n",
    "print(\"Training Accuracy:\", np.sum(training_results == y_train.get()) / y_train.shape[0])\n",
    "print(\"Test Accuracy:\", np.sum(test_results == y_test.get()) / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d55f0-6aa9-4c4e-9391-cd0c3dfc94e5",
   "metadata": {},
   "source": [
    "Another highly interpretable model we can use is decision trees. But the problem with that is that it fails to perform at scale. It's a recursive tree algorithm, which are not embarassingly parallel. But we can use boosting methods to parallelize it, such as XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e5958-3b9a-4b20-bd2a-6bc66b12ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.experimental.xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(pd.DataFrame(X_train.get()), pd.DataFrame(y_train.get()))\n",
    "dtest = xgb.DMatrix(pd.DataFrame(X_test.get()), pd.DataFrame(y_test.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1473c1a-5716-4b18-86da-283784cc0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"eta\": 0.3,\n",
    "    \"max_depth\": 3,\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 2,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "}\n",
    "steps = 20\n",
    "\n",
    "# Create dict for evaluation results\n",
    "evals_result = dict()\n",
    "\n",
    "# Run training\n",
    "model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    steps,\n",
    "    evals=[(dtrain, \"train\")],\n",
    "    evals_result=evals_result\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(f'Evals results:\\n{evals_result}')\n",
    "\n",
    "# Predict results\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "# Print prediction results\n",
    "print(f'Prediction results:\\n{prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296d92f-4f40-4d68-be16-eb75a8eae3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = prediction[1].to_numpy()\n",
    "y_result[y_result >= 0.5] = 1\n",
    "y_result[y_result < 0.5] = 0\n",
    "np.sum(y_result == y_test.get()) / y_result.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb20cd-5c75-46cf-8fc9-973056f72fdf",
   "metadata": {},
   "source": [
    "We can also perform random forests, another embarassingly parallel algorithm that can utilize running decision trees per Ray Actors across a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33c520-e2fe-4b9e-b0aa-8b79290a728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from ray.util.joblib import register_ray\n",
    "register_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c3d60-bf18-4eb9-a1b6-b721532a90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with joblib.parallel_backend('ray'):\n",
    "    model = RandomForestClassifier(n_jobs=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b547c-1a07-4c76-b3cb-a2f9ea010b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train.get(), y_train.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bfd472-ae99-4623-bc36-9e7271e8db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('ray'):\n",
    "    print(\"Training Accuracy:\", model.score(X_train.get(), y_train.get()))\n",
    "    print(\"Test Accuracy:\", model.score(X_test.get(), y_test.get()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9abe6-4da8-4170-aa2d-abed8bf0d0d4",
   "metadata": {},
   "source": [
    "Recall the question we asked earlier, if we can \"learn\" weather patterns as well, especially in places where it is inhabitable and not much weather stations availiable. Could we generalize these sparse areas of data? The features of our previous design matrix involved day of year, precipitation, temperature maximum, temperature minimum, temperature average, temperature range, latitude, longitude, and elevation. But looking at new/unknown locations limits the amount of features we can deal with. As a starting point, we can create a meshgrid of longitudes and latitudes. Then train and predict precipitation based purely by location and day of year. We lose some motivating features such as temperature and elevation. Given terrain data for elevation, we could easily merge it via Modin and maybe get better preicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156855e8-9053-4dd5-8f25-6e0c41ba0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nps.unique(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d113b6-d1c3-4c5c-93f3-c714ef8eff10",
   "metadata": {},
   "source": [
    "## Modeling Weather by Time with Time Series (NumS)\n",
    "Next, we can model and predict our data to figure out what it has to do with time. Weather often has seasonal patterns. Let's focus in on a specific weather station for this tutorial, which will be station USC00040693. This weather station is located on UC Berkeley campus [(Google Maps)](https://www.google.com/maps/place/37°52'27.8%22N+122°15'38.2%22W/@37.8744024,-122.2618614,17z/data=!3m1!4b1!4m5!3m4!1s0x0:0x0!8m2!3d37.8744!4d-122.2606). Through Modin, we can quickly parse out what type of data this weather station records. Let's revisit our exploratory data analysis skills to unreveal what the data is able to provide us in terms of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2977e-7f34-4bc1-95ac-1393aa3272be",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory[inventory[\"ID\"] == 'USC00040693']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef263e-249a-4f05-abfa-79e27844e4e7",
   "metadata": {},
   "source": [
    "Data will vary a lot between stations, some stations may record as little as just precipitation. Coincidentally, this weather station is located on UC Berkeley campus and has been recording data of various elements for quite a while. The more interesting data values recorded in this station other than temperature and precipitation are the WT** data types. According to the [documentation](https://docs.opendata.aws/noaa-ghcn-pds/readme.html) they are mapped to:\n",
    "\n",
    "* **01 = Fog, ice fog, or freezing fog (may include heavy fog)**\n",
    "* 02 = Heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "* **03 = Thunder**\n",
    "* **04 = Ice pellets, sleet, snow pellets, or small hail**\n",
    "* **05 = Hail (may include small hail)**\n",
    "* 06 = Glaze or rime\n",
    "* 07 = Dust, volcanic ash, blowing dust, blowing sand, or blowing obstruction\n",
    "* **08 = Smoke or haze**\n",
    "* 09 = Blowing or drifting snow\n",
    "* 10 = Tornado, waterspout, or funnel cloud\n",
    "* **11 = High or damaging winds**\n",
    "* 12 = Blowing spray\n",
    "* 13 = Mist\n",
    "* **14 = Drizzle**\n",
    "* 15 = Freezing drizzl\n",
    "* 16 = **Rain (may include freezing rain, drizzle, and freezing drizzle)**\n",
    "* 17 = Freezing rain\n",
    "* 18 = Snow, snow pellets, snow grains, or ice crystals\n",
    "* 19 = Unknown source of precipitation\n",
    "* 21 = Ground fog\n",
    "* 22 = Ice fog or freezing fog\n",
    "\n",
    "Unfortunately, plotting these special elements out shows us there is nothing interesting going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467efe85-d8aa-42bc-8166-3ef990239a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_weather_elements = inventory[inventory[\"ID\"] == 'USC00040693'][[\"ELEMENT\", \"FIRSTYEAR\", \"LASTYEAR\"]]\n",
    "berkeley_weather_elements = berkeley_weather_elements[berkeley_weather_elements[\"ELEMENT\"].isin([\"TMAX\", \"TMIN\", \"PRCP\"])] #override to save time\n",
    "berkeley_time_series = {}\n",
    "\n",
    "for _, rows in berkeley_weather_elements.iterrows():\n",
    "    element, firstyear, lastyear = rows\n",
    "    berkeley_time_series[element] = df_filter_vector('USC00040693', element, local=local, custom_years=(lastyear-20, lastyear - 1)) #only grabbing the last 20 years for effiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55240bb3-d352-4ba6-8a07-8d8f69e77ffa",
   "metadata": {},
   "source": [
    "We can plot the data in the last 3 years (Plotting all of the data will take a long time due to the bottleneck of Matplotlib's serial performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257746f-4e46-4098-b02c-9a3ade425b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_time_series[\"TMAX\"].tail(365 * 3).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd22b8-d258-4c11-86ba-1af0678d81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_time_series[\"TMIN\"].tail(365 * 3).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b1bd5-fe11-4484-8b6a-977fb1d6c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_time_series[\"PRCP\"].tail(365 * 3).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a7013-be1a-496e-94d2-0a06793a1a9f",
   "metadata": {},
   "source": [
    "We can start modeling with simple linear regression. Although it wont give us the most accurate model and predictions due to weather data being seasonal and sinusoidal, it is at least a good starting point and baseline for other model's performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007ba4b-f4cf-408c-b161-56a17fb0d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.models.glms import LinearRegression\n",
    "y_train = berkeley_time_series[\"TMIN\"].iloc[-4000:-1000].to_numpy().reshape(-1) \n",
    "y_test = berkeley_time_series[\"TMIN\"].iloc[-1000:].to_numpy().reshape(-1) \n",
    "\n",
    "#convert to NumS arrays, assigning to y since we want X to be time, y to be TMIN (data value)\n",
    "y_train = nps.array(y_train)\n",
    "y_test = nps.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d5366-45fc-4948-8c6b-cf45254a68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = nps.arange(0, y_train.shape[0], 1).reshape(-1, 1).astype(np.double)\n",
    "X_test = nps.arange(y_train.shape[0], y_train.shape[0] + y_test.shape[0], 1).reshape(-1, 1).astype(np.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45dd4a-f4bc-4ca8-90b8-3d21fc2fc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.get(), y_train.get());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e93e2-3304-4f2c-832b-ecbfbe52fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training RMSE:\", rmse(y_train, model.predict(X_train)))\n",
    "print(\"Test RMSE:\", rmse(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29c883-4174-4f19-9fc8-33c7adef443e",
   "metadata": {},
   "source": [
    "Model also reveals to use that there is only a slight positive trend in data with the slope obtained from, indicating that there is indeed climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea10374-cf98-466b-b262-a0b30f28b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._beta.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da823c-d6c9-4f09-a221-018c84d04416",
   "metadata": {},
   "source": [
    "We see that temperature has a sinusoudal pattern. Before doing more modeling, let's stack all the years on top of each other to get a design matrix of temperature data where the rows are year, columns are day of year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29baaa-829b-4112-940d-fd592794ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_tmax_design_matrix = design_matrix_time_series_stack('USC00040693', 'TMAX', years, local=local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174f940-249b-416a-8a5c-7cce613ca06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(berkeley_tmax_design_matrix.shape[0]):\n",
    "    plt.plot(berkeley_tmax_design_matrix[i].get());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e5ca70-e57d-49c6-a72c-7bcdca6d6c1d",
   "metadata": {},
   "source": [
    "With this design matrix, we can run Principal Component Analysis (PCA) with NumS's SVD solver to observe abnormalities in the data. Ideally, it should be reducable to one dimension since all the rows should be nearly identical, with some variance, as shown in the scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc7eb0-45ef-4bc3-a740-0fa49f69c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.core import linalg\n",
    "from nums.core import apaplication_manager\n",
    "nps_app_inst = application_manager.instance()\n",
    "\n",
    "U, S, V = linalg.svd(nps_app_inst, nps.nan_to_num(berkeley_tmax_design_matrix.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cb8ff-27cf-4a4b-8cbb-74416b478766",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S[:25].get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2518278-da9e-400b-a799-5f0b6742352f",
   "metadata": {},
   "source": [
    "But when plotting the projection vectors, we observe something different. We see that near day 50, there is a sharp spike. This is usually the time that leap days occur, indicating that there is missing data or inconsistesies. We will see how this affects us later. (***TODO: fix after time series model are done***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687a7bc-72ab-4bfe-86b1-7daa2171ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    plt.plot(U.T[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70663bd-01fd-479e-b3be-403216b3b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: (Daniel) Factor in autoregressive models once done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567e456-53cf-44a7-b0ef-025660c1af45",
   "metadata": {},
   "source": [
    "## End\n",
    "To stop and shutdown the notebook properly, we can free memory and shutdown Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8658e-9122-408b-b98a-337c8188a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4636e-8bca-4323-a561-d268cb28dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
