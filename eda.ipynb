{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2fccdb",
   "metadata": {},
   "source": [
    "# AWS-ASDI\n",
    "EDA on the NOAA Global Historical Climatology Network Daily Dataset. This dataset contains over 200 years worth of climate data, and we will analyze the dataset through the use of data science tools that are scalable (Modin, NumS, and Ray). Because the tools we use are scalable, we are able to run the same code on a laptop and cluster of nodes. Before getting started, make sure you set `num_cpus` in `ray_init()` to the number of physical cores on the CPU running this notebook for optimal performance, otherwise Ray might automatically set it to logical cores that include hyperthreading. Additionally, for cluster setup, the only thing that needs to be changed is properly setting the correct parameters in `ray.init()` and now the same notebook should be able to scale.\n",
    "\n",
    "Confirm everything is installed through:\n",
    "```sh\n",
    "pip3 install -r requirements\n",
    "```\n",
    "\n",
    "When done with the notebook, you can properly shutdown Ray to free all processes and memory using:\n",
    "```python\n",
    "ray.shutdown()\n",
    "```\n",
    "For convenience, this cell is included at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4ff10e-262b-4d8b-ac34-ca3be714016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "local = True #Change this flag to False to download data off of s3 storage directly\n",
    "n_jobs = 32 #this should be set to the number of logical cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76c32f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 01:59:16,965\tINFO services.py:1274 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.init(ignore_reinit_error=True, num_cpus=n_jobs, _temp_dir=\"/home/brian/external/aws-asdi/ray_temp\"); # Remove or reassign _temp_dir if not enough disk storage on device\n",
    "#ray.init(ignore_reinit_error=True, address=\"auto\") #ray.init() config for cluster setup\n",
    "import modin.pandas as pd\n",
    "import pandas\n",
    "from nums import numpy as nps\n",
    "from nums.core import settings\n",
    "from nums.experimental import nums_modin\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220554d0-780c-4126-afce-3b2700f9e952",
   "metadata": {},
   "source": [
    "Setting for NumS to assign head and worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4883c20-fcb9-4276-9ab2-eb4eec0f9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings.cluster_shape = (len(ray.nodes())-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f085a",
   "metadata": {},
   "source": [
    "## Downloading the Data and File Organization\n",
    "* [Data](https://registry.opendata.aws/noaa-ghcn/)\n",
    "* [Documentation](https://docs.opendata.aws/noaa-ghcn-pds/readme.html)\n",
    "\n",
    "We can use AWS CLI to speed things up by downloading it locally. More info about installation is provided [here](https://aws.amazon.com/cli/).\n",
    "\n",
    "To view files in terminal/command line:\n",
    "```sh\n",
    "aws s3 ls s3://noaa-ghcn-pds/ --no-sign-request\n",
    "```\n",
    "\n",
    "To download locally (*about 100GB of free data is needed on disk*), we can download directly to our `data/` directory:\n",
    "```sh\n",
    "mkdir data\n",
    "aws s3 cp --recursive s3://noaa-ghcn-pds/csv/ --no-sign-request data\n",
    "```\n",
    "\n",
    "If downloading will take too much storage space, we can run this notebook completely off of memory, by setting the `local` tag to `False` in the cell directly above. Some `.txt` files will be directly loaded from S3, as they are relatively small and are just lookup tables/directories for the `.csv` files.\n",
    "\n",
    "File directory should look like this:\n",
    "```\n",
    "├── data/\n",
    "├── figures/\n",
    "├── eda.ipynb\n",
    "├── README.md\n",
    "└── requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8980a19",
   "metadata": {},
   "source": [
    "Here, we test downloading a `.csv` file directly from the AWS S3 bucket to memory. Ensure this is working before running the next cells. This notebook will do all of the data cleaning of DataFrames in memory. Because the S3 bucket is public, there should be no need to insert any AWS/S3 credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8634708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.55 s, sys: 3.57 s, total: 9.12 s\n",
      "Wall time: 39.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climate_2020 = pd.read_csv('s3://noaa-ghcn-pds/csv/2020.csv', header=None)\n",
    "climate_2020.columns = [\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"]\n",
    "climate_2020[\"YEAR/MONTH/DAY\"] = pd.to_datetime(climate_2020[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "del climate_2020 # delete from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46b7801-28bc-47ae-934e-6b6b0ffe2694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 s, sys: 7.53 s, total: 31 s\n",
      "Wall time: 58.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climate_2020 = pandas.read_csv('s3://noaa-ghcn-pds/csv/2020.csv', header=None)\n",
    "climate_2020.columns = [\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"]\n",
    "climate_2020[\"YEAR/MONTH/DAY\"] = pd.to_datetime(climate_2020[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "del climate_2020 # delete from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9fc31-9ef3-4573-87be-cfd1e2924f67",
   "metadata": {},
   "source": [
    "The times below are tested on a single node setup. As we can see, the speedups are not reliable as it is dependent on our connection speed between your network to the S3 bucket (located on us-east-1):\n",
    "\n",
    "**modin**\n",
    "```\n",
    "CPU times: user 11.6 s, sys: 2.99 s, total: 14.6 s\n",
    "Wall time: 1min 42s\n",
    "```\n",
    "**pandas**\n",
    "```\n",
    "CPU times: user 25.3 s, sys: 3.68 s, total: 29 s\n",
    "Wall time: 1min 22s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7ffb1f-e0e2-4bae-af31-889dd354c4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.85 s, sys: 1.01 s, total: 2.86 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#skip if not downloaded locally\n",
    "climate_2020 = pd.read_csv('data/2020.csv', header=None)\n",
    "climate_2020.columns = [\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"]\n",
    "climate_2020[\"YEAR/MONTH/DAY\"] = pd.to_datetime(climate_2020[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "del climate_2020 # delete from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96fa1877-842b-49f8-9014-a42359b204c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 s, sys: 3.57 s, total: 21.1 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#skip if not downloaded locally\n",
    "climate_2020 = pandas.read_csv('data/2020.csv', header=None)\n",
    "climate_2020.columns = [\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"]\n",
    "climate_2020[\"YEAR/MONTH/DAY\"] = pd.to_datetime(climate_2020[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "del climate_2020 # delete from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcfe34c-5290-4f82-83d6-40823af77f0d",
   "metadata": {},
   "source": [
    "We can try the same cell again, but with data already stored to the local filesystem for a more reliable benchmark. As we can see, a significant speedup for a file that is 1 GB in size:\n",
    "\n",
    "**modin**\n",
    "```\n",
    "CPU times: user 1.3 s, sys: 518 ms, total: 1.82 s\n",
    "Wall time: 2.92 s\n",
    "```\n",
    "**pandas**\n",
    "```\n",
    "CPU times: user 17 s, sys: 2.16 s, total: 19.2 s\n",
    "Wall time: 18.7 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcd926",
   "metadata": {},
   "source": [
    "## Global Variables and DataFrames\n",
    "Some variables and DataFrames that will be useful in parsing out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f65d6",
   "metadata": {},
   "source": [
    "For inventory, data is stored like this:\n",
    "\n",
    "```\n",
    "Variable\tColumns\tType\n",
    "ID\t1-11\tCHARACTER\n",
    "LATITUDE\t13-20\tREAL\n",
    "LONGITUDE\t22-30\tREAL\n",
    "ELEMENT\t32-35\tCHARACTER\n",
    "FIRSTYEAR\t37-40\tINTEGER\n",
    "LASTYEAR\t42-45\tINTEGER\n",
    "```\n",
    "\n",
    "Sample query\n",
    "```\n",
    "ACW00011604  17.1167  -61.7833 TMAX 1949 1949\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f05fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>FIRSTYEAR</th>\n",
       "      <th>LASTYEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704607</th>\n",
       "      <td>ZI000067983</td>\n",
       "      <td>-20.2000</td>\n",
       "      <td>32.6160</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>1951</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704608</th>\n",
       "      <td>ZI000067983</td>\n",
       "      <td>-20.2000</td>\n",
       "      <td>32.6160</td>\n",
       "      <td>TAVG</td>\n",
       "      <td>1962</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704609</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1951</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704610</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>1951</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704611</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>1951</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704612 rows x 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  LATITUDE  LONGITUDE ELEMENT  FIRSTYEAR  LASTYEAR\n",
       "0       ACW00011604   17.1167   -61.7833    TMAX       1949      1949\n",
       "1       ACW00011604   17.1167   -61.7833    TMIN       1949      1949\n",
       "2       ACW00011604   17.1167   -61.7833    PRCP       1949      1949\n",
       "3       ACW00011604   17.1167   -61.7833    SNOW       1949      1949\n",
       "4       ACW00011604   17.1167   -61.7833    SNWD       1949      1949\n",
       "...             ...       ...        ...     ...        ...       ...\n",
       "704607  ZI000067983  -20.2000    32.6160    PRCP       1951      2020\n",
       "704608  ZI000067983  -20.2000    32.6160    TAVG       1962      2020\n",
       "704609  ZI000067991  -22.2170    30.0000    TMAX       1951      1990\n",
       "704610  ZI000067991  -22.2170    30.0000    TMIN       1951      1990\n",
       "704611  ZI000067991  -22.2170    30.0000    PRCP       1951      1990\n",
       "\n",
       "[704612 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventory = pd.read_fwf('s3://noaa-ghcn-pds/ghcnd-inventory.txt', widths=[12, 9, 10, 4, 5, 5], header=None)\n",
    "inventory.columns = [\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEMENT\", \"FIRSTYEAR\", \"LASTYEAR\"]\n",
    "inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9982d",
   "metadata": {},
   "source": [
    "For stations, the data is formatted like this:\n",
    "```\n",
    "Variable\tColumns\tType\tExample\n",
    "ID\t1-11\tCharacter\tEI000003980\n",
    "LATITUDE\t13-20\tReal\t55.3717\n",
    "LONGITUDE\t22-30\tReal\t-7.3400\n",
    "ELEVATION\t32-37\tReal\t21.0\n",
    "STATE\t39-40\tCharacter\n",
    "NAME\t42-71\tCharacter\tMALIN HEAD\n",
    "GSN FLAG\t73-75\tCharacter\tGSN\n",
    "HCN/CRN FLAG\t77-79\tCharacter\n",
    "WMO ID\t81-85\tCharacter\t03980\n",
    "```\n",
    "\n",
    "A sample query:\n",
    "```\n",
    "AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fdb13d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN FLAG</th>\n",
       "      <th>HCN/CRN FLAG</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>10.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>19.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.5170</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.3640</td>\n",
       "      <td>10.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.6510</td>\n",
       "      <td>26.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118487</th>\n",
       "      <td>ZI000067969</td>\n",
       "      <td>-21.0500</td>\n",
       "      <td>29.3670</td>\n",
       "      <td>861.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WEST NICHOLSON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118488</th>\n",
       "      <td>ZI000067975</td>\n",
       "      <td>-20.0670</td>\n",
       "      <td>30.8670</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MASVINGO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118489</th>\n",
       "      <td>ZI000067977</td>\n",
       "      <td>-21.0170</td>\n",
       "      <td>31.5830</td>\n",
       "      <td>430.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUFFALO RANGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118490</th>\n",
       "      <td>ZI000067983</td>\n",
       "      <td>-20.2000</td>\n",
       "      <td>32.6160</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHIPINGE</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118491</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>457.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BEITBRIDGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67991.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118492 rows x 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0       ACW00011604   17.1167   -61.7833       10.1   NaN   \n",
       "1       ACW00011647   17.1333   -61.7833       19.2   NaN   \n",
       "2       AE000041196   25.3330    55.5170       34.0   NaN   \n",
       "3       AEM00041194   25.2550    55.3640       10.4   NaN   \n",
       "4       AEM00041217   24.4330    54.6510       26.8   NaN   \n",
       "...             ...       ...        ...        ...   ...   \n",
       "118487  ZI000067969  -21.0500    29.3670      861.0   NaN   \n",
       "118488  ZI000067975  -20.0670    30.8670     1095.0   NaN   \n",
       "118489  ZI000067977  -21.0170    31.5830      430.0   NaN   \n",
       "118490  ZI000067983  -20.2000    32.6160     1132.0   NaN   \n",
       "118491  ZI000067991  -22.2170    30.0000      457.0   NaN   \n",
       "\n",
       "                         NAME GSN FLAG HCN/CRN FLAG   WMO ID  \n",
       "0       ST JOHNS COOLIDGE FLD      NaN          NaN      NaN  \n",
       "1                    ST JOHNS      NaN          NaN      NaN  \n",
       "2         SHARJAH INTER. AIRP      GSN          NaN  41196.0  \n",
       "3                  DUBAI INTL      NaN          NaN  41194.0  \n",
       "4              ABU DHABI INTL      NaN          NaN  41217.0  \n",
       "...                       ...      ...          ...      ...  \n",
       "118487         WEST NICHOLSON      NaN          NaN  67969.0  \n",
       "118488               MASVINGO      NaN          NaN  67975.0  \n",
       "118489          BUFFALO RANGE      NaN          NaN  67977.0  \n",
       "118490               CHIPINGE      GSN          NaN  67983.0  \n",
       "118491             BEITBRIDGE      NaN          NaN  67991.0  \n",
       "\n",
       "[118492 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations = pd.read_fwf('s3://noaa-ghcn-pds/ghcnd-stations.txt', widths=[12, 9, 10, 7, 3, 31, 4, 4, 6], header=None)\n",
    "stations.columns = [\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"STATE\", \"NAME\", \"GSN FLAG\", \"HCN/CRN FLAG\", \"WMO ID\"]\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e0230f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AC</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AG</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AJ</td>\n",
       "      <td>Azerbaijan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>WI</td>\n",
       "      <td>Western Sahara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>WQ</td>\n",
       "      <td>Wake Island [United States]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>WZ</td>\n",
       "      <td>Swaziland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>ZA</td>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>ZI</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows x 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    code                       country\n",
       "0     AC          Antigua and Barbuda \n",
       "1     AE         United Arab Emirates \n",
       "2     AF                   Afghanistan\n",
       "3     AG                      Algeria \n",
       "4     AJ                   Azerbaijan \n",
       "..   ...                           ...\n",
       "214   WI               Western Sahara \n",
       "215   WQ   Wake Island [United States]\n",
       "216   WZ                    Swaziland \n",
       "217   ZA                       Zambia \n",
       "218   ZI                     Zimbabwe \n",
       "\n",
       "[219 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes = pd.read_csv(\"s3://noaa-ghcn-pds/ghcnd-countries.txt\", delimiter=\"\\n\", header=None)[0].str.extract('(?P<code>.{2})(?P<country>.{0,})')\n",
    "country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b3f2fbe-b404-4967-b6c4-ea2c021974af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "elements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "all_elements = list(inventory[\"ELEMENT\"].unique())\n",
    "years = list(range(1763, 2022))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e90cf",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "Some helpful functions in helping with EDA and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a2af10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_loader(year, local=False):\n",
    "    if local:\n",
    "        df = pd.read_csv('data/' + str(year) + '.csv', header=None)\n",
    "    else:\n",
    "        df = pd.read_csv('s3://noaa-ghcn-pds/csv/' + str(year) + '.csv', header=None)\n",
    "    df.columns = [\"ID\", \"YEAR/MONTH/DAY\", \"ELEMENT\", \"DATA VALUE\", \"M-FLAG\", \"Q-FLAG\", \"S-FLAG\", \"OBS-TIME\"]\n",
    "    df[\"YEAR/MONTH/DAY\"] = pd.to_datetime(df[\"YEAR/MONTH/DAY\"], format=\"%Y%m%d\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a5c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_filter(df, _id, element):\n",
    "    return df.loc[(df[\"ID\"] == _id) & (df[\"ELEMENT\"] == element)][[\"YEAR/MONTH/DAY\", \"DATA VALUE\"]].set_index(\"YEAR/MONTH/DAY\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786e2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as df_filter, but a vector of ALL the data\n",
    "def df_filter_vector(_id, element, local=False, custom_years=None):\n",
    "    df_vector = pd.DataFrame(columns=[\"DATA VALUE\"])\n",
    "    if custom_years:\n",
    "        years = range(custom_years[0], custom_years[1] + 1)\n",
    "    for year in tqdm(years):\n",
    "        if local:\n",
    "            df = df_filter(dfs[year], _id, element)\n",
    "        else:\n",
    "            df = df_filter(df_loader(year), _id, element)\n",
    "            \n",
    "        if df_vector.empty:\n",
    "            df_vector = df\n",
    "        else:\n",
    "            df_vector = df_vector.append(df)\n",
    "    return df_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "684675cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statements are replaced with tqdm functions\n",
    "# MatPlotLib runs on single thread, we can parallelize plotting by using Ray\n",
    "@ray.remote\n",
    "def plotter(df, _id, element, save=False, year=None):\n",
    "    station_name = stations.loc[stations['ID'] == _id][\"NAME\"].item()\n",
    "    df = df_filter(df, _id, element)\n",
    "    \n",
    "    if df.empty:\n",
    "        tqdm.write(element + \" data on \" + str(year) + \" for \" + station_name + \" with id: \" + _id + \" is empty. Plotting is skipped.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(df)\n",
    "    plt.title(element + \" Data at \" + station_name)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    #plt.xlim() #figure out a way to set limit of year\n",
    "    plt.ylim(-100, 400)\n",
    "    if save:\n",
    "        directory = \"figures/\" + _id + \"/\" + element\n",
    "        try:\n",
    "            # Create target directory\n",
    "            os.makedirs(directory)\n",
    "            tqdm.write(f\"Directory \" + directory + \" Created \")\n",
    "        except FileExistsError:\n",
    "            tqdm.write(f\"Directory \" + directory + \" already exists. File \" + directory + \"/\" + str(year) + \".png has been saved successfully.\")\n",
    "        \n",
    "        plt.savefig(directory + \"/\" + str(year) + \".png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4198a365-e811-4fd8-84e5-3ea7824c3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, expected):\n",
    "    \"\"\"\n",
    "    Computes the root mean squared error to evaluate models/predictions. It can accept all the datatypes used\n",
    "    in this notebook\n",
    "    \"\"\"\n",
    "    if type(actual) != type(expected):\n",
    "        raise TypeError(\"actual and expected must be the same types\")\n",
    "    if type(actual) == np.ndarray:\n",
    "        return np.sqrt(np.mean((expected - actual) ** 2))\n",
    "    elif type(actual) == nums.core.array.blockarray.BlockArray:\n",
    "        return nps.sqrt(nps.mean((expected - actual) ** 2))\n",
    "    elif type(actual) == modin.pandas.dataframe.DataFrame or type(actual) == pandas.core.frame.DataFrame:\n",
    "        #TODO\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ae83b2-e957-4111-89d5-84ed4019cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_matrix(years, elements, target=None, convert_nps=False, local=False):\n",
    "    \"\"\"\n",
    "    Set target to your \"y\" predictor. If y has NaNs or missing values, we will drop the data row.\n",
    "    \"\"\"\n",
    "    df_design = pd.DataFrame()\n",
    "    \n",
    "    for year in tqdm(years):\n",
    "        if local:\n",
    "            df = dfs[year]\n",
    "        else:\n",
    "            df = df_loader(year)\n",
    "            \n",
    "\n",
    "        if target[0] not in df[\"ELEMENT\"].unique():\n",
    "            continue\n",
    "\n",
    "        df = df[df['ELEMENT'].isin(elements)]\n",
    "        df = pd.pivot_table(df, index=[\"ID\", \"YEAR/MONTH/DAY\"], columns=\"ELEMENT\", values=\"DATA VALUE\").reset_index(level=[0,1])\n",
    "        df = df.merge(stations[[\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"]], how='inner', on='ID')\n",
    "        \n",
    "        if target:\n",
    "            df = df.dropna(subset=target)\n",
    "        df = df.dropna() #TODO add a flag to toggle this\n",
    "        \n",
    "        \n",
    "        df[\"YEAR/MONTH/DAY\"] = df[\"YEAR/MONTH/DAY\"].apply(lambda x: pd.Period(x, freq='D').day_of_year)\n",
    "        df[\"TMAX\"] = df[\"TMAX\"] / 10\n",
    "        df[\"TMIN\"] = df[\"TMIN\"] / 10\n",
    "        df[\"AVG\"] = (df[\"TMAX\"] + df[\"TMIN\"]) / 2\n",
    "        df[\"RANGE\"] = df[\"TMAX\"] - df[\"TMIN\"]\n",
    "        #df = df.drop([\"ID\"], axis=1)\n",
    "        #df = df.astype(float)\n",
    "        \n",
    "        if df_design.empty:\n",
    "            df_design = df\n",
    "        else:\n",
    "            df_design = df_design.append(df)\n",
    "        \n",
    "        \n",
    "    \n",
    "    if convert_nps:\n",
    "        return nps.array(result.to_numpy().astype(np.double))\n",
    "    return df_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "483b3fe1-ca62-485e-be8f-9a80d73a4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_matrix_time_series_stack(_id, element, years, convert_nps=True, local=False):\n",
    "    \"\"\"\n",
    "    Inputs are station ID and element\n",
    "    Output is a design matrix of time series per year stacked on top of each other.\n",
    "    rows are year of data collected, \n",
    "    cols are day of the year\n",
    "    \n",
    "    returns NumS array or Pandas DataFrame\n",
    "    \"\"\"\n",
    "    df_design = pd.DataFrame(columns=pd.date_range(start=\"2020-01-01\", end=\"2020-12-31\").strftime('%m-%d'))\n",
    "    station_name = stations.loc[stations['ID'] == _id][\"NAME\"].item()\n",
    "    \n",
    "    for year in tqdm(years):\n",
    "        if local:\n",
    "            df = dfs[year]\n",
    "        else:\n",
    "            try:\n",
    "                df = df_loader(year)\n",
    "            except ClientError:\n",
    "                tqdm.write(str(year) + \".csv doesn't exist on remote, addition to design matrix is skipped.\")\n",
    "                continue\n",
    "            \n",
    "        df = df_filter(df, _id, element)\n",
    "        if df.empty:\n",
    "            tqdm.write(element + \" data on \" + str(year) + \" for \" + station_name + \" with id: \" + _id + \" is empty. Addition to design matrix is skipped.\")\n",
    "            continue\n",
    "        df.index = df.index.strftime('%m-%d')\n",
    "        df.columns = [year]\n",
    "        df = df.T\n",
    "        df_design = df_design.append(df)\n",
    "\n",
    "\n",
    "    df_design.index.name = None\n",
    "    if convert_nps:\n",
    "        #return nums_modin.from_modin(df_design) #uncomment this once bug gets fixed. \n",
    "        return nps.array(df_design.to_numpy().astype(np.double))\n",
    "    return df_design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ac851",
   "metadata": {},
   "source": [
    "## Loading all the DataFrames to Memory\n",
    "We can also store all the dataframes into memory, given that there is enough RAM. Dataset is ~100GB, so something more that that should be fine. (Uses 200GB+ of memory on this system)\n",
    "\n",
    "Skip this cell if running on less RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8076d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbe5e565f964d1683db98accf7a5866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "dfs = {}\n",
    "for year in tqdm(years):\n",
    "    dfs[year] = df_loader(year, local=local) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a497a87",
   "metadata": {},
   "source": [
    "Loading all the dataframes from local storage to memory gives us these times:\n",
    "\n",
    "**modin**:\n",
    "```\n",
    "CPU times: user 2min 48s, sys: 54 s, total: 3min 42s\n",
    "Wall time: 5min 14s\n",
    "```\n",
    "\n",
    "**pandas**\n",
    "```\n",
    "CPU times: user 24min 22s, sys: 5min 39s, total: 30min 2s\n",
    "Wall time: 29min 32s\n",
    "```\n",
    "\n",
    "As we can see, Modin gives us ~6x speedup, which is impressive considering that ~100GB is being transferred and loaded onto memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813a33e",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Visualizations and understanding the data. We can plot how many data points each country has collected in their weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344933a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_data_freq = pd.DataFrame()\n",
    "years.reverse() #reverse, so NaNs can be filled in to missing/old data\n",
    "\n",
    "for year in tqdm(years):\n",
    "    station_data_freq[year] = dfs[year].groupby(dfs[year][\"ID\"].str.slice(stop=2))[\"DATA VALUE\"].sum()\n",
    "\n",
    "years.reverse()\n",
    "station_data_freq = station_data_freq.fillna(0).T\n",
    "station_data_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0930480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in tqdm(country_codes.iterrows()): \n",
    "    if row[0] in station_data_freq.T.columns:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = station_data_freq.T[row[0]].plot();\n",
    "        ax.set_xlabel(\"Years\")\n",
    "        ax.set_ylabel(\"Number of Data Points\")\n",
    "        plt.savefig((\"figures/datasamples/\" + row[1] + \".png\").replace(\" \", \"_\"))\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbbf11-c850-4cde-9332-5ac03c7afb21",
   "metadata": {},
   "source": [
    "For example, this is number of data points for United States station.\n",
    "\n",
    "![](figures/datasamples/_United_States_.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac37dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points = []\n",
    "for year in tqdm(years):\n",
    "    sample_points.append(len(dfs[year].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec4eb9",
   "metadata": {},
   "source": [
    "Note that there is a dip at the end. That is due to this dataset being collected and updated live. Thus, it includes the year 2021 (the current year making this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b74cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot(years, sample_points);\n",
    "plt.title(\"Number of Data Points Collected Per Year\")\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Data Points\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massively plots elements per year for each id\n",
    "\"\"\"\n",
    "for _id in tqdm(stations[\"ID\"]):\n",
    "    for element in elements:\n",
    "        for year in years:\n",
    "            plotter.remote(dfs[year], _id, element, save=True, year=year)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e367b53",
   "metadata": {},
   "source": [
    "We can also analyze what type of weather data is being collected. Here is a plot of the top 25 weather data being collected ranked by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242404c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory[\"ELEMENT\"].value_counts().head(25).plot(kind=\"bar\");\n",
    "plt.title(\"Top Weather Data Type Collected\")\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096acdbb-7520-456e-bcf2-912db014162d",
   "metadata": {},
   "source": [
    "We can also analyze location of each station by plotting their longitude and latitude to give us a familiar outline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c2e5d-b682-48f4-8422-3623dc5952a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Scatterplot of Weather Stations\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.scatter(x=stations['LONGITUDE'], y=stations['LATITUDE'], s=0.9, alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280928e6-c1f6-45cd-9ca9-53f959621ea4",
   "metadata": {},
   "source": [
    "Using plotly, we can make this more interactive and visualize the density of these weather stations. Note that for some libraries that need to use Pandas, we can insert `._to_pandas()` to revert a modin dataframe to pandas. First we can plot a density heatmap to show where each station is located. It also shows us that there are more weather stations in densly populated areas.\n",
    "\n",
    "Source/motivation: https://medium.com/plotly/how-to-create-2d-and-3d-interactive-weather-maps-in-python-and-r-77ddd53cca8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63586c2d-8723-4239-b0cb-80c42308c05c",
   "metadata": {},
   "source": [
    "Since plotly plots can take up space, it won't be uploaded to the repo. But after running, it can be viewed through this link after it's saved to `.html` file.\n",
    "\n",
    "Link: http://127.0.0.1:8888/files/station_density.html\n",
    "\n",
    "(If link doesn't work, make sure port number is same. It is set to the jupyter default port 8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9ed09-97a9-4814-b487-91070951e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_mapbox(stations._to_pandas(), lat='LATITUDE', lon='LONGITUDE', radius=5,\n",
    "                        center=dict(lat=0, lon=180), zoom=0,\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"station_density.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bae8b6-5d0a-4b42-bcf6-dae1da79c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = design_matrix([2010], [\"TMAX\", \"TMIN\"], target=[\"TMAX\"], local=True)\n",
    "df[\"TMAX\"] = df[\"TMAX\"] / 10\n",
    "df[\"TMIN\"] = df[\"TMIN\"] / 10\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b100c6-c837-4a17-be70-3cbb2e83f674",
   "metadata": {},
   "source": [
    "We can also plot the TMAX density of temperature.\n",
    "\n",
    "Link: http://127.0.0.1:8888/files/tmax_density.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237813c-7a4d-4f50-bc91-1433123c88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "temp = df[df[\"YEAR/MONTH/DAY\"] == 1]._to_pandas()\n",
    "fig = go.Figure(go.Densitymapbox(lat=temp.LATITUDE, lon=temp.LONGITUDE, z=temp.TMAX,\n",
    "                                 radius=1))\n",
    "fig.update_layout(mapbox_style=\"stamen-terrain\", mapbox_center_lon=180)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"tmax_density.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668a96c-4c14-4413-837d-0087ee3e365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces, one for each slider step\n",
    "for day in tqdm(np.arange(1, 367, 1)):\n",
    "    temp = df[df[\"YEAR/MONTH/DAY\"] == day]._to_pandas()\n",
    "    fig.add_trace(\n",
    "        go.Densitymapbox(lat=temp.LATITUDE, lon=temp.LONGITUDE, z=temp.TMAX,\n",
    "                                 radius=10, visible=False, \n",
    "                        name=\"Temperature Max on Day \"+ str(day))\n",
    "    )\n",
    "\n",
    "# Make 10th trace visible\n",
    "fig.data[0].visible = True\n",
    "\n",
    "# Create and add slider\n",
    "steps = []\n",
    "\n",
    "for i in tqdm(range(len(fig.data))):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)},\n",
    "              {\"title\": \"Slider switched to Day: \" + str(i)}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "    currentvalue={\"prefix\": \"Frequency: \"},\n",
    "    pad={\"t\": 50},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders,\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
    "    mapbox_style=\"stamen-terrain\", \n",
    "    mapbox_center_lon=180\n",
    ")\n",
    "\n",
    "fig.write_html(\"tmax_density_timelapse.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aff52d",
   "metadata": {},
   "source": [
    "Next, let's analyze and plot data from a weather station. We will analyze the station USC00040693 in Berkeley, CA [(Google Maps)](https://www.google.com/maps/place/37°52'27.8%22N+122°15'38.2%22W/@37.8744024,-122.2618614,17z/data=!3m1!4b1!4m5!3m4!1s0x0:0x0!8m2!3d37.8744!4d-122.2606). Through Pandas/modin, we can figure out which elements of weather data this station records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de564ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory[inventory[\"ID\"] == 'USC00040693']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835889c7",
   "metadata": {},
   "source": [
    "Data will vary a lot between stations, some stations may record as little as just precipitation. Coincidentally, this weather station is located on UC Berkeley campus and has been recording data of various elements for quite a while. The more interesting data values recorded in this station other than temperature and precipitation are the WT** data types. According to the [documentation](https://docs.opendata.aws/noaa-ghcn-pds/readme.html) they are mapped to:\n",
    "\n",
    "* **01 = Fog, ice fog, or freezing fog (may include heavy fog)**\n",
    "* 02 = Heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "* **03 = Thunder**\n",
    "* **04 = Ice pellets, sleet, snow pellets, or small hail**\n",
    "* **05 = Hail (may include small hail)**\n",
    "* 06 = Glaze or rime\n",
    "* 07 = Dust, volcanic ash, blowing dust, blowing sand, or blowing obstruction\n",
    "* **08 = Smoke or haze**\n",
    "* 09 = Blowing or drifting snow\n",
    "* 10 = Tornado, waterspout, or funnel cloud\n",
    "* **11 = High or damaging winds**\n",
    "* 12 = Blowing spray\n",
    "* 13 = Mist\n",
    "* **14 = Drizzle**\n",
    "* 15 = Freezing drizzl\n",
    "* 16 = **Rain (may include freezing rain, drizzle, and freezing drizzle)**\n",
    "* 17 = Freezing rain\n",
    "* 18 = Snow, snow pellets, snow grains, or ice crystals\n",
    "* 19 = Unknown source of precipitation\n",
    "* 21 = Ground fog\n",
    "* 22 = Ice fog or freezing fog\n",
    "\n",
    "Unfortunately, plotting these special elements out shows us there is nothing interesting going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199e0d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: Make dataloading faster by batching operations into a design matrix with features, if possible. \n",
    "#Currently 7-8s per iteration, at most 12 min per vector. See if this improves when scaled.\n",
    "berkeley_weather_elements = inventory[inventory[\"ID\"] == 'USC00040693'][[\"ELEMENT\", \"FIRSTYEAR\", \"LASTYEAR\"]]\n",
    "berkeley_weather_elements = berkeley_weather_elements[berkeley_weather_elements[\"ELEMENT\"].isin([\"TMAX\", \"TMIN\", \"PRCP\"])] #override to save time\n",
    "berkeley_time_series = {}\n",
    "\n",
    "for _, rows in berkeley_weather_elements.iterrows():\n",
    "    element, firstyear, lastyear = rows\n",
    "    berkeley_time_series[element] = df_filter_vector('USC00040693', element, local=local, custom_years=(lastyear-20, lastyear - 1)) #only grabbing the last 20 years for effiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d856d3a",
   "metadata": {},
   "source": [
    "We can plot data in the last 3 years, (plotting all of the data will take a long time due to the bottleneck of matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_time_series[\"TMAX\"].tail(365 * 3).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f480921",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_time_series[\"TMIN\"].tail(365 * 3).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_time_series[\"PRCP\"].tail(365 * 3).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc95bba",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "#TODO: Refactor modeling portion to a separate notebook after done experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05a0f8",
   "metadata": {},
   "source": [
    "## Simple Modeling with FFT\n",
    "Using NumPy's FFT module to forecast seasonal weather patterns by extrapolation. A function will be extrapolated with FFT on the training dataset and then we can evaluate how well we extrapolated using root mean squared error. We can also tune the variables such as `ratio`, `predict`, and `n_harm` which represents ratio between training and test set, number of days to predict in test set, and frequency of FFT respectively. Tuning these will help avoiding overfitting.\n",
    "\n",
    "* Source: https://gist.github.com/tartakynov/83f3cd8f44208a1856ce\n",
    "* StackOverflow Post: https://stackoverflow.com/questions/4479463/using-fourier-analysis-for-time-series-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_extrapolation(X, n_predict, n_harm=100):\n",
    "    n = X.size\n",
    "    t = np.arange(0, n)\n",
    "    p = np.polyfit(t, X, 1)            # find linear trend in x\n",
    "    X_notrend = X - p[0] * t           # detrended x\n",
    "    X_freqdom = np.fft.fft(X_notrend)  # detrended x in frequency domain\n",
    "    f = np.fft.fftfreq(n)              # frequencies\n",
    "    indexes = list(range(n))\n",
    "    indexes.sort(key = lambda i: np.absolute(f[i]))\n",
    " \n",
    "    t = np.arange(0, n + n_predict)\n",
    "    restored_sig = np.zeros(t.size)\n",
    "    for i in indexes[:1 + n_harm * 2]:\n",
    "        ampli = np.absolute(X_freqdom[i]) / n   # amplitude\n",
    "        phase = np.angle(X_freqdom[i])          # phase\n",
    "        restored_sig += ampli * np.cos(2 * np.pi * f[i] * t + phase)\n",
    "    return restored_sig + p[0] * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 6      # Ratio will be train:test = ratio:1\n",
    "predict = 366 * 3 # Number of future days to predict\n",
    "n_harm = 15\n",
    "\n",
    "X_train = berkeley_time_series[\"TMIN\"].sort_index().iloc[-ratio * predict:-predict].to_numpy().reshape(-1) \n",
    "X_test = berkeley_time_series[\"TMIN\"].sort_index().iloc[-predict:].to_numpy().reshape(-1) \n",
    "\n",
    "extrapolation = fourier_extrapolation(X_train, predict, n_harm=n_harm)\n",
    "plt.figure(figsize=(50, 10))\n",
    "plt.plot(np.arange(0, extrapolation.size), extrapolation, 'r', label = 'extrapolation', linewidth=3)\n",
    "plt.plot(np.arange(0, X_train.size), X_train, 'b', label = r'training data', linewidth = 3)\n",
    "plt.plot(np.arange(X_train.size, extrapolation.size), X_test, 'g', label = r'test data')\n",
    "plt.legend()\n",
    "plt.savefig(\"fft_berkeley_tmin.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069975e4-19bb-4c55-a1fb-ebfcbbed0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training RMSE:\", rmse(extrapolation[:X_train.size], X_train))\n",
    "print(\"Test RMSE:\", rmse(extrapolation[-X_test.size:], X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef7aa5",
   "metadata": {},
   "source": [
    "For more sporadic data like precipitation, it's much more harder to predict and identify trends with FFT, thus giving a large RMSE value compared to temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be699a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 4\n",
    "predict = 366 * 2 # Number of future days to predict\n",
    "n_harm = 500\n",
    "\n",
    "X_train = berkeley_time_series[\"PRCP\"].sort_index().iloc[-ratio * predict:-predict].to_numpy().reshape(-1) \n",
    "X_test = berkeley_time_series[\"PRCP\"].sort_index().iloc[-predict:].to_numpy().reshape(-1) \n",
    "\n",
    "extrapolation = fourier_extrapolation(X_train, predict, n_harm=n_harm)\n",
    "plt.figure(figsize=(50, 10))\n",
    "plt.plot(np.arange(0, extrapolation.size), extrapolation, 'r', label = 'extrapolation', linewidth=3)\n",
    "plt.plot(np.arange(0, X_train.size), X_train, 'b', label = r'training data', linewidth = 3)\n",
    "plt.plot(np.arange(X_train.size, extrapolation.size), X_test, 'g', label = r'test data')\n",
    "plt.legend()\n",
    "plt.savefig(\"fft_berkeley_prcp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9a94e-fef7-4b20-80c2-ae99f6b08b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training RMSE:\", rmse(extrapolation[:X_train.size], X_train))\n",
    "print(\"Test RMSE:\", rmse(extrapolation[-X_test.size:], X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8b17a",
   "metadata": {},
   "source": [
    "## Linear Regression with NumS and Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e04dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.models.glms import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b20d07-8420-49c6-93a1-c6f429685c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = berkeley_time_series[\"TMIN\"].iloc[-4000:-1000].to_numpy().reshape(-1) \n",
    "y_test = berkeley_time_series[\"TMIN\"].iloc[-1000:].to_numpy().reshape(-1) \n",
    "\n",
    "#convert to NumS arrays, assigning to y since we want X to be time, y to be TMIN (data value)\n",
    "y_train = nps.array(y_train)\n",
    "y_test = nps.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc20b5-e900-4f72-b4bd-d0241ac1cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = nps.arange(0, y_train.shape[0], 1).reshape(-1, 1).astype(np.double)\n",
    "X_test = nps.arange(y_train.shape[0], y_train.shape[0] + y_test.shape[0], 1).reshape(-1, 1).astype(np.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43507adb-bfca-4edb-880d-2834e34bc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.get(), y_train.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bafcb7c-eb0b-49e4-af15-a8388e7ecefd",
   "metadata": {},
   "source": [
    "We can attempt to fit the time series vector of temperature with NumS linear regression. Nothing interesting should be happening, but shows us how NumS can solve linear regression in a simple case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7be0b-01a6-42ce-93c6-895363cf107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "m = model._beta.get()\n",
    "b = model._beta0.get()\n",
    "\n",
    "plt.plot(X_train.get(), y_train.get())\n",
    "plt.plot(X_train.get(), m * X_train.get() + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5c8ea-b8ba-4876-96a5-074dbcd3507d",
   "metadata": {},
   "source": [
    "Model also reveals to use that there is only a slight positive trend in data with the slope obtained from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f870b0-8381-4ad1-a3ad-56df651338a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._beta.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720b0ec-8ba8-4971-ab09-d2bf22c237ec",
   "metadata": {},
   "source": [
    "## Creating the Design Matrix for Predicting/Classifying Precipitation\n",
    "Next, we can create a design matrix with multiple features as elements and perform classification. In the next few cells, we will try to classify whether or not snow will fall from data all around the world! We will use several features to help us determine if there is precipitation or not such as temperature, location, elevation, and day of the year. Note that what we are doing is *time-independent* opposed to the previous example with time series. We will explore various popular models in classical machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94be09d-75a0-4545-a9b6-6db43895e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_elements = [\"PRCP\", \"TMAX\", \"TMIN\"]\n",
    "data = design_matrix(years[-10:], test_elements, target=[\"PRCP\"], convert_nps=False, local=local)\n",
    "data = data[data[\"PRCP\"] >= 0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0110dc4-7523-48ea-b097-c5a75067621d",
   "metadata": {},
   "source": [
    "Going back to our EDA, we can visualize the design matrix through plotly by plotting the precipitaton during a certain day of the year. Here, we can plot the precipication on January 1st 2020. We observe that there is a lot of rain happening at higher elevations (mountains) in the US. There are also data at other places, but recalling our previous station density map, it's not as dense in other places.\n",
    "\n",
    "http://127.0.0.1:8888/files/prcp_dmatrix_density.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314ec37-6d65-43ea-9e92-7892cfe64506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot = design_matrix([2020], test_elements, target=[\"PRCP\"], convert_nps=False, local=local)\n",
    "plot = plot[plot[\"PRCP\"] >= 0]\n",
    "plot = plot[(plot[\"YEAR/MONTH/DAY\"] == 1) & (plot[\"PRCP\"] > 0)]\n",
    "fig = go.Figure(go.Densitymapbox(lat=plot.LATITUDE, lon=plot.LONGITUDE, z=plot.PRCP,\n",
    "                                 radius=10))\n",
    "fig.update_layout(mapbox_style=\"stamen-terrain\", mapbox_center_lon=180)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"prcp_dmatrix_density.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b9ba0-3e4c-4bf4-85e4-35b5c8c6c149",
   "metadata": {},
   "source": [
    "Here, we will perform a train-test-split based on station IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f355d-f14f-4207-aebd-337fa06fcfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.random.permutation(data[\"ID\"].unique())\n",
    "split = int(ids.shape[0] * .8)\n",
    "train_ids = ids[:split]\n",
    "test_ids = ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fe2ce-9b7c-4901-bc87-8a97d6afce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data[\"ID\"].isin(train_ids)]\n",
    "test = data[data[\"ID\"].isin(test_ids)]\n",
    "#'TMAX', 'TMIN','AVG', 'RANGE',\n",
    "X_train = train[['YEAR/MONTH/DAY', 'TMAX', 'TMIN','AVG', 'RANGE', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].to_numpy().astype(np.double)\n",
    "y_train = train['PRCP'].to_numpy().astype(np.double)\n",
    "X_test = test[['YEAR/MONTH/DAY', 'TMAX', 'TMIN','AVG', 'RANGE', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].to_numpy().astype(np.double)\n",
    "y_test = test['PRCP'].to_numpy().astype(np.double)\n",
    "\n",
    "# NumS arrays must maintain block shapes, code below ensures they are shaped correctly.\n",
    "X_train = nps.array(X_train)\n",
    "y_train = nps.array(y_train)\n",
    "X_test = nps.array(X_test)\n",
    "y_test = nps.array(y_test)\n",
    "y_train = y_train.reshape(block_shape=(X_train.block_shape[0],))\n",
    "y_test = y_test.reshape(block_shape=(X_test.block_shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afecb15-fd91-4a6e-9b65-01114d294434",
   "metadata": {},
   "source": [
    "## Linear Regression with NumS and Multiple Features\n",
    "Next , we'll use linear regression with NumS to see if we can classify precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9fa94-14ff-4ddc-a25e-1aa8334d9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.models.glms import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b44802-be7a-48d4-8f40-0ae594bac9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = model.predict(X_train).get()\n",
    "test_results = model.predict(X_test).get()\n",
    "\n",
    "print(\"Training RMSE:\", rmse(training_results, y_train.get()))\n",
    "print(\"Testing RMSE\", rmse(test_results, y_test.get()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a14ffe0-372f-40b1-ba92-4b32398996d6",
   "metadata": {},
   "source": [
    "## Logistic Regression with NumS and Multiple Features\n",
    "A continutation from the previous module, we can also use NumS logistic regression to classify if it will precipitation or not.\n",
    "\n",
    "We can set y to be a binary variable, but we need to do some hacks since advanced indexing is not supported yet on NumS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e95f8-1744-41e1-9333-cd1264ea2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_train and y_test to binary variable\n",
    "y_train_block_shape = y_train.block_shape\n",
    "y_train = y_train.get()\n",
    "y_train[y_train > 0] = 1.0\n",
    "y_train = nps.array(y_train)\n",
    "y_train = y_train.reshape(block_shape=y_train_block_shape)\n",
    "\n",
    "\n",
    "y_test_block_shape = y_test.block_shape\n",
    "y_test = y_test.get()\n",
    "y_test[y_test > 0] = 1.0\n",
    "y_test = nps.array(y_test)\n",
    "y_test = y_test.reshape(block_shape=y_test_block_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4812929-7895-4af5-9a88-9b797e704619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.models.glms import LogisticRegression\n",
    "\n",
    "# since matrix could be singular, a mask is added on to X_train to help invert it\n",
    "mask = nps.random.rand(X_train.shape[0], X_train.shape[1]) * 0.00001\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train + mask, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be51d57-22e6-47a6-b88f-b62abc0fee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = model.predict(X_train).get()\n",
    "test_results = model.predict(X_test).get()\n",
    "\n",
    "print(\"Training Accuracy:\", np.sum(training_results == y_train.get()) / y_train.shape[0])\n",
    "print(\"Test Accuracy:\", np.sum(test_results == y_test.get()) / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761040b8-ca4a-41c2-9a2b-8f34d61f11bb",
   "metadata": {},
   "source": [
    "## Decision Trees with Scikitlearn\n",
    "We can also use Scikit-learn's Decision Tree to train and predict if there is precipitation or not. We can also use that model to plot out the features it splits on, giving us more insight on what the motivating features are. For the purposes of plotting, the depth is limited to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda923bd-f705-4e26-8cf1-23b2cd043a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train.get(), y_train.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68ac98-d9e8-4abf-a936-236f1cddabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy:\", model.score(X_train.get(), y_train.get()))\n",
    "print(\"Test Accuracy:\", model.score(X_test.get(), y_test.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d240c4d-1e8a-4f26-88c8-681ee5ccc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['YEAR/MONTH/DAY', 'TMAX', 'TMIN','AVG', 'RANGE', 'LATITUDE', 'LONGITUDE', 'ELEVATION']\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,10), dpi=500);\n",
    "\n",
    "tree.plot_tree(model, feature_names=columns, class_names=[\"NO PRCP\", \"PRCP\"], filled=True);\n",
    "\n",
    "fig.savefig('decision_tree_prcp.png', dpi=500, transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab636b4f-7ef4-479c-b971-6d6cc9cd01bb",
   "metadata": {},
   "source": [
    "## XGBoost with Modin (Experimental)\n",
    "We can also perform classification with XGBoost within the expermeintal package in Modin.\n",
    "\n",
    "* Documentation: https://modin.readthedocs.io/en/latest/modin_xgboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4b145-0b30-4faa-967d-9190aa668b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.experimental.xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42f7f7-802c-44db-9c85-78c4f55d4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(pd.DataFrame(X_train.get()), pd.DataFrame(y_train.get()))\n",
    "dtest = xgb.DMatrix(pd.DataFrame(X_test.get()), pd.DataFrame(y_test.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05eebc0-07fc-4711-8c81-beb8f2c17b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"eta\": 0.3,\n",
    "    \"max_depth\": 3,\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 2,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "}\n",
    "steps = 20\n",
    "\n",
    "# Create dict for evaluation results\n",
    "evals_result = dict()\n",
    "\n",
    "# Run training\n",
    "model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    steps,\n",
    "    evals=[(dtrain, \"train\")],\n",
    "    evals_result=evals_result\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(f'Evals results:\\n{evals_result}')\n",
    "\n",
    "# Predict results\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "# Print prediction results\n",
    "print(f'Prediction results:\\n{prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e75c0-8b4c-438f-900d-7ff7c13e847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = prediction[1].to_numpy()\n",
    "y_result[y_result >= 0.5] = 1\n",
    "y_result[y_result < 0.5] = 0\n",
    "np.sum(y_result == y_test.get()) / y_result.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97e889-981e-416d-b4b9-cc4c882c3115",
   "metadata": {},
   "source": [
    "## Random Forests with Ray Backend\n",
    "We can also perform random forest classifier with Ray by replacing the parallel backend joblib with Ray. This will automatically distribute Decision Trees across multiple threads/clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f8f77-dcf4-4743-a4a2-c7b75ffe6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from ray.util.joblib import register_ray\n",
    "register_ray()\n",
    "\n",
    "with joblib.parallel_backend('ray'):\n",
    "    model = RandomForestClassifier(n_jobs=n_jobs) #TODO, change n_jobs to match the number of threads in cluster\n",
    "model.fit(X_train.get(), y_train.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e665d89-aa5b-4534-9ace-0944d37d0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy:\", model.score(X_train.get(), y_train.get()))\n",
    "print(\"Test Accuracy:\", model.score(X_test.get(), y_test.get()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bc1bf-f4e2-4cc7-9b76-ebf5e2b52eea",
   "metadata": {},
   "source": [
    "## Modeling (Again but with Classifying Snow)\n",
    "Next, we will repeat the same models but with snow. But as we see when we graph it, we are limited to only US and Canada data, as other countries don't support snow data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32588c21-6edc-4587-a3fd-2c08e6f317eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_elements = [\"SNOW\", \"TMAX\", \"TMIN\"]\n",
    "data = design_matrix(years[-10:], test_elements, target=[\"SNOW\"], convert_nps=False, local=local)\n",
    "data = data[data[\"SNOW\"] >= 0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f576ac5-1b48-47cb-bd1b-fa25423b040f",
   "metadata": {},
   "source": [
    "As mentioned before, we can plot the data, and graph it onto a map to see that Snow data is only available on US and Canada\n",
    "\n",
    "http://127.0.0.1:8888/files/snow_dmatrix_density.html\n",
    "\n",
    "And for comparison, we see that not all precipation data is snow. An interesting observation is that sometimes snow in mountain happens at higher elevations:\n",
    "\n",
    "http://127.0.0.1:8888/files/prcp_dmatrix_density.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c702216-b86d-4f71-a459-db698493dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = design_matrix([2020], test_elements, target=[\"SNOW\"], convert_nps=False, local=local)\n",
    "plot = plot[plot[\"SNOW\"] >= 0]\n",
    "plot = plot[(plot[\"YEAR/MONTH/DAY\"] == 1) & (plot[\"SNOW\"] > 0)]\n",
    "fig = go.Figure(go.Densitymapbox(lat=plot.LATITUDE, lon=plot.LONGITUDE, z=plot.SNOW,\n",
    "                                 radius=10))\n",
    "fig.update_layout(mapbox_style=\"stamen-terrain\", mapbox_center_lon=180)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"snow_dmatrix_density.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d732a8-7cc4-4245-8144-a982de54c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.random.permutation(data[\"ID\"].unique())\n",
    "split = int(ids.shape[0] * .8)\n",
    "train_ids = ids[:split]\n",
    "test_ids = ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afb0d3-9a72-4eb2-b4f4-522894981263",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data[\"ID\"].isin(train_ids)]\n",
    "test = data[data[\"ID\"].isin(test_ids)]\n",
    "#'TMAX', 'TMIN','AVG', 'RANGE',\n",
    "X_train = train[['YEAR/MONTH/DAY', 'TMAX', 'TMIN','AVG', 'RANGE', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].to_numpy().astype(np.double)\n",
    "y_train = train['SNOW'].to_numpy().astype(np.double)\n",
    "X_test = test[['YEAR/MONTH/DAY', 'TMAX', 'TMIN','AVG', 'RANGE', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].to_numpy().astype(np.double)\n",
    "y_test = test['SNOW'].to_numpy().astype(np.double)\n",
    "\n",
    "# NumS arrays must maintain block shapes, code below ensures they are shaped correctly.\n",
    "X_train = nps.array(X_train)\n",
    "y_train = nps.array(y_train)\n",
    "X_test = nps.array(X_test)\n",
    "y_test = nps.array(y_test)\n",
    "y_train = y_train.reshape(block_shape=(X_train.block_shape[0],))\n",
    "y_test = y_test.reshape(block_shape=(X_test.block_shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb457061-45e4-4ee9-9aa3-e2436a0eadf4",
   "metadata": {},
   "source": [
    "## Linear Regression with Multiple Features (Snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8cc522-aff2-4174-9d2a-e93df61c6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.models.glms import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5ade9-5afa-4c85-bccf-e74646709030",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = model.predict(X_train).get()\n",
    "test_results = model.predict(X_test).get()\n",
    "\n",
    "print(\"Training RMSE:\", rmse(training_results, y_train.get()))\n",
    "print(\"Testing RMSE\", rmse(test_results, y_test.get()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1eccd-15eb-4959-b832-d9e0cbb8f74d",
   "metadata": {},
   "source": [
    "## Logistic Regression with Multiple Features (Snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72065a84-e61a-4028-a441-412461d0c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_train and y_test to binary variable\n",
    "y_train_block_shape = y_train.block_shape\n",
    "y_train = y_train.get()\n",
    "y_train[y_train > 0] = 1.0\n",
    "y_train = nps.array(y_train)\n",
    "y_train = y_train.reshape(block_shape=y_train_block_shape)\n",
    "\n",
    "\n",
    "y_test_block_shape = y_test.block_shape\n",
    "y_test = y_test.get()\n",
    "y_test[y_test > 0] = 1.0\n",
    "y_test = nps.array(y_test)\n",
    "y_test = y_test.reshape(block_shape=y_test_block_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332a4c0-017a-49b0-bf0e-3d8a1eef25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.models.glms import LogisticRegression\n",
    "\n",
    "# since matrix could be singular, a mask is added on to X_train to help invert it\n",
    "mask = nps.random.rand(X_train.shape[0], X_train.shape[1]) * 0.00001\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train + mask, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece0be7-10e1-4286-9381-93791368d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = model.predict(X_train).get()\n",
    "test_results = model.predict(X_test).get()\n",
    "\n",
    "print(\"Training Accuracy:\", np.sum(training_results == y_train.get()) / y_train.shape[0])\n",
    "print(\"Test Accuracy:\", np.sum(test_results == y_test.get()) / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a56ef7c-e8bb-4e45-a25e-045bf911de85",
   "metadata": {},
   "source": [
    "## Decision Trees with Scikitlearn (Snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609181e2-6b34-4de4-9827-ee3fb3c10742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=4)\n",
    "model.fit(X_train.get(), y_train.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51832cf-2c94-4fda-bb01-701fff8340a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy:\", model.score(X_train.get(), y_train.get()))\n",
    "print(\"Test Accuracy:\", model.score(X_test.get(), y_test.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5c97d-2718-4101-9e67-e3496bfc37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['YEAR/MONTH/DAY', 'TMAX', 'TMIN','AVG', 'RANGE', 'LATITUDE', 'LONGITUDE', 'ELEVATION']\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,10), dpi=500);\n",
    "\n",
    "tree.plot_tree(model, feature_names=columns, class_names=[\"NO SNOW\", \"SNOW\"], filled=True);\n",
    "\n",
    "fig.savefig('decision_tree_snow.png', dpi=500, transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4d65e-4b24-46ed-8552-79304da4d325",
   "metadata": {},
   "source": [
    "## XGBoost with Modin (Snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c255c-0476-4c01-8119-00f1c3ee767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.experimental.xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9619d05-1d9b-4fba-b892-258b3426e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(pd.DataFrame(X_train.get()), pd.DataFrame(y_train.get()))\n",
    "dtest = xgb.DMatrix(pd.DataFrame(X_test.get()), pd.DataFrame(y_test.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95415c94-a2da-42b1-8317-ccdbce29eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"eta\": 0.3,\n",
    "    \"max_depth\": 3,\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 2,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "}\n",
    "steps = 20\n",
    "\n",
    "# Create dict for evaluation results\n",
    "evals_result = dict()\n",
    "\n",
    "# Run training\n",
    "model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    steps,\n",
    "    evals=[(dtrain, \"train\")],\n",
    "    evals_result=evals_result\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(f'Evals results:\\n{evals_result}')\n",
    "\n",
    "# Predict results\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "# Print prediction results\n",
    "print(f'Prediction results:\\n{prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ac866-4c59-46e8-a974-620d7ca9a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = prediction[1].to_numpy()\n",
    "y_result[y_result >= 0.5] = 1\n",
    "y_result[y_result < 0.5] = 0\n",
    "np.sum(y_result == y_test.get()) / y_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785ff17-db06-4b63-9f60-f8f2587f0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from ray.util.joblib import register_ray\n",
    "register_ray()\n",
    "\n",
    "with joblib.parallel_backend('ray'):\n",
    "    model = RandomForestClassifier(n_jobs=n_jobs) #TODO, change n_jobs to match the number of threads in cluster\n",
    "model.fit(X_train.get(), y_train.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c795bb8-0dad-4cf9-8b38-f54d0058b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy:\", model.score(X_train.get(), y_train.get()))\n",
    "print(\"Test Accuracy:\", model.score(X_test.get(), y_test.get()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be4c36-b8bb-4045-9289-303e781610ae",
   "metadata": {},
   "source": [
    "## Using Models to Learn New Locations\n",
    "Using a model, can we possible \"learn\" a new location that doesn't have a weather station? We can create a meshgrid of longitude and latitude, just by location, we will try to clasify snow. Of coures, we have to discard features like temperature. We also have to discard elevation, but if there happens to be detailed terrain data for longitude, latitude, and elevation, it could definitely help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3cb00-0eb2-4469-9a3b-ceb9466efeac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-90, 90, 10000)\n",
    "y = np.linspace(-180, 180, 10000)\n",
    "\n",
    "geo_data = []\n",
    "\n",
    "for i in tqdm(range(x.shape[0])):\n",
    "    for j in range(y.shape[0]):\n",
    "        geo_data.append((x[i], y[j]))\n",
    "        \n",
    "new_location_df = pd.DataFrame(geo_data, columns=[\"lat\", \"lon\"])\n",
    "new_location_df[\"day\"] = 1\n",
    "new_location_df = new_location_df.reindex(columns=['day', 'lat', 'lon'])\n",
    "location_grid = new_location_df.to_numpy().astype(np.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18c3fb-be35-4b28-af30-7bf2d4f4b433",
   "metadata": {},
   "source": [
    "For this, we will only predict snow at unknown locations on Januaury 1st, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9acff-fe09-4504-a1ce-6c32dec9a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[[\"YEAR/MONTH/DAY\", \"LATITUDE\", \"LONGITUDE\", \"SNOW\"]]\n",
    "df = df[df[\"YEAR/MONTH/DAY\"] == 1]\n",
    "X = df[[\"YEAR/MONTH/DAY\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "y = df[\"SNOW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd815bf-7893-4002-9818-e5aee4c57c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from ray.util.joblib import register_ray\n",
    "register_ray()\n",
    "\n",
    "with joblib.parallel_backend('ray'):\n",
    "    model = RandomForestClassifier(n_jobs=n_jobs)\n",
    "model.fit(X, y)\n",
    "print(\"Training Accuracy:\", model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad9959-e247-48a9-8af9-f5e400104058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict new locations\n",
    "snow = model.predict(location_grid)\n",
    "new_location_df[\"snow\"] = snow\n",
    "new_location_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626c4e8-4a73-4b8e-8fb1-7fadb8ee6194",
   "metadata": {},
   "source": [
    "For plotting purposes, we can just filter out where it snows. It'll be obvious in the plot where we set the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2e6dd-c36e-4a68-a981-e2f32af4ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_location_df = new_location_df[new_location_df[\"snow\"] == 1]\n",
    "new_location_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb2203-2ab6-4d99-aa2e-733b33213db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Densitymapbox(lon=new_location_df.lon, lat=new_location_df.lat, radius=5))\n",
    "fig.update_layout(mapbox_style=\"stamen-terrain\", mapbox_center_lon=180)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"new_locations_snow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723585af-f519-4791-b6e8-b2e084675c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = data[[\"YEAR/MONTH/DAY\", \"LATITUDE\", \"LONGITUDE\", \"SNOW\"]]\n",
    "actual = actual[(actual[\"YEAR/MONTH/DAY\"] == 1) & (actual[\"SNOW\"] > 0)]\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40943115-9b81-4559-9fc9-f81213a76a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Densitymapbox(lon=actual.LONGITUDE, lat=actual.LATITUDE,\n",
    "                                 radius=5))\n",
    "fig.update_layout(mapbox_style=\"stamen-terrain\", mapbox_center_lon=180)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.write_html(\"actual_snow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09ea1c",
   "metadata": {},
   "source": [
    "## PCA with NumS\n",
    "We can stack years of data on top of each other to give a design matrix of years as cols/features, and day of the year as rows/data point. Running PCA can tell us if there is any abnormality weather between the years. Continuing with the data from Berkeley, this is what the data shows:\n",
    "\n",
    "Source/inspriation: https://towardsdatascience.com/the-pca-trick-with-time-series-d40d48c69d28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092749f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "berkeley_tmax_design_matrix = design_matrix_time_series_stack('USC00040693', 'TMAX', years, local=local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bb362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(berkeley_tmax_design_matrix.shape[0]):\n",
    "    plt.plot(berkeley_tmax_design_matrix[i].get());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73650555",
   "metadata": {},
   "source": [
    "Seems like the data has one significant feature, which should be expected. Also notice a jump near day 50, indicating that this is around February, when leap days occur, causing inconsistencies in data. There are also some abnormalities in the other dimensions, potentially indicating shifts/differences in climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "U, S, V = np.linalg.svd(np.nan_to_num(berkeley_tmax_design_matrix.T.get()), full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    plt.plot(U.T[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2192ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nums.core import linalg\n",
    "from nums.core import application_manager\n",
    "nps_app_inst = application_manager.instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "U, S, V = linalg.svd(nps_app_inst, nps.nan_to_num(berkeley_tmax_design_matrix.T))\n",
    "U.touch() #for timing purposes\n",
    "S.touch()\n",
    "V.touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(U.T[0, :].get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72250255",
   "metadata": {},
   "source": [
    "Scree Plot of the first 10 singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191247d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(S[:10].get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f1b04",
   "metadata": {},
   "source": [
    "Let's repeat the same with precipication data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c58308",
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_prcp_design_matrix = design_matrix_time_series_stack('USC00040693', 'PRCP', years, local=local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(berkeley_prcp_design_matrix.shape[0]):\n",
    "    plt.plot(berkeley_prcp_design_matrix[i].get());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = linalg.svd(nps_app_inst, nps.nan_to_num(berkeley_prcp_design_matrix.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a583a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    plt.plot(U.T[i, :].get());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214aed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scree plot\n",
    "plt.plot(S[:10].get());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a2d00",
   "metadata": {},
   "source": [
    "# End Here\n",
    "When done, run this so you can delete the large dictionary of dataframes and shutdown Ray properly to free all processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a97696-4564-4912-9908-b7635e261d51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3497/2780944092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4234e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
